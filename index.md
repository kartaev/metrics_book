<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# Лекция 01. Эндогенность

## Предпосылки линейной регрессии (условия Гаусса-Маркова)

Для корректного применения метода наименьших квадратов необходимо выполнение четырех основных условий для всех наблюдений $i = 1, \ldots, n$:

1. **$E(\varepsilon_i) = 0$** - несистематичная ошибка
2. **$\text{Var}(\varepsilon_i) = \sigma^2$** - гомоскедастичность
3. **$\text{Cov}(\varepsilon_i, \varepsilon_j) = 0$** для всех $i \neq j$ - отсутствие автокорреляции
4. **Случайный член должен быть распределен независимо от объясняющих переменных**

Нарушение четвертой предпосылки приводит к проблеме эндогенности.

## Определение и сущность эндогенности

### Основные определения

**Эндогенные переменные** - объясняющие переменные в $X$, которые связаны с ошибкой уравнения $\varepsilon$. Это представляет собой нарушение предпосылки теоремы Гаусса-Маркова.

**Экзогенные переменные** - переменные, некоррелированные с ошибкой.

### Математическое представление эндогенности

Эндогенность возникает при нарушении одного из следующих эквивалентных условий:

$$\text{cov}(X, \varepsilon) = 0$$
$$E(\varepsilon|X) = 0$$
$$\text{plim}_{n \to \infty} \frac{X'\varepsilon}{n} = 0$$

## Модель со стохастическими регрессорами

### Основные предпосылки

Для линейной модели со стохастическими регрессорами необходимо выполнение следующих условий:

1. **Линейность по параметрам**: $Y_i = \beta_1 + \beta_2 x_i^{(2)} + \beta_3 x_i^{(3)} + \cdots + \beta_k x_i^{(k)} + \varepsilon_i$
2. **Независимость наблюдений**: $\{(x_i^{(2)}, \ldots, x_i^{(k)}, y_i), i = 1, \ldots, n\}$ независимы и одинаково распределены
3. **Конечность моментов**: $X_i^{(2)}, \ldots, X_i^{(k)}, Y_i$ имеют ненулевые конечные четвертые моменты
4. **Условное математическое ожидание ошибок**: $E[\varepsilon_i|x_i^{(2)}, \ldots, x_i^{(k)}] = 0$
5. **Отсутствие мультиколлинеарности**: с вероятностью единица отсутствует чистая мультиколлинеарность

### Теорема о состоятельности

При выполнении предпосылок 1-5 МНК-оценки коэффициентов модели множественной регрессии являются состоятельными и асимптотически нормальными.

## Последствия эндогенности

### Основные проблемы

При наличии эндогенности оценки МНК становятся:

- **Смещенными**
- **Несостоятельными**


### Математическое обоснование

Для несостоятельности МНК-оценок при эндогенности справедливо:

$$\text{plim}_{n \to \infty} \hat{\beta} = \beta + \text{plim}_{n \to \infty} \left(\frac{X'X}{n}\right)^{-1} \frac{X'\varepsilon}{n} \neq \beta$$

Это означает, что даже при увеличении размера выборки оценки не стремятся к истинным значениям параметров.

## Причины возникновения эндогенности

### 1. Пропущенная переменная

**Суть проблемы:** Некоторая важная объясняющая переменная, коррелированная с включенными регрессорами, исключена из модели.

**Классический пример - уравнение заработной платы:**

$$\ln \text{wage}_i = \alpha + \beta_1 S_i + \beta_2 \text{EXP}_i + \beta_3 \text{EXP}_i^2 + \beta_4 \text{ability}_i + \varepsilon_i$$

где $\text{ability}_i$ - ненаблюдаемая переменная способностей.

**Проблема интерпретации:** Если переменная способностей не включена в модель, но коррелирует с образованием, то оценка отдачи от образования будет завышена. Модель оценивает не причинно-следственную связь, а просто говорит о том, что люди с более высоким уровнем образования имеют более высокую заработную плату.

### 2. Ошибка измерения

Ошибки в измерении объясняющих переменных приводят к корреляции между регрессором и случайной ошибкой.

### 3. Одновременность и обратная причинно-следственная связь

**Определение:** Регрессор $X$ влияет на $Y$, но одновременно $Y$ влияет на $X$.

**Практические примеры двусторонних причинно-следственных связей:**

- Затраты на образование и ВВП
- Уровень преступности и количество полицейских
- Заболеваемость и количество медработников


### 4. Автокорреляция с лаговой зависимой переменной

Включение лаговых значений зависимой переменной при наличии автокорреляции ошибок также создает проблему эндогенности.

## Метод инструментальных переменных

### Определение инструментов

**Инструменты (Z)** - переменные, удовлетворяющие двум ключевым требованиям:

1. **Релевантность:** Сильная корреляция с эндогенным регрессором $X$
2. **Валидность (экзогенность):** Некоррелированность с ошибкой: $\text{plim}_{n \to \infty} \frac{Z'\varepsilon}{n} = 0$

### Оценивание методом IV

При точной идентификации (количество инструментов равно количеству регрессоров: $m = k$) оценка инструментальных переменных имеет вид:

$$\hat{\beta}_{IV} = (Z'X)^{-1}Z'Y$$

### Доказательство состоятельности

Состоятельность IV-оценки доказывается следующим образом:

$$\hat{\beta}_{IV} = (Z'X)^{-1}Z'Y = (Z'X)^{-1}Z'(X\beta + \varepsilon) = \beta + \left(\frac{Z'X}{n}\right)^{-1}\frac{Z'\varepsilon}{n}$$

Поскольку $\text{plim}_{n \to \infty} \frac{Z'\varepsilon}{n} = 0$, получаем $\text{plim}_{n \to \infty} \hat{\beta}_{IV} = \beta$.

## Двухшаговый метод наименьших квадратов (2SLS)

### Применение и алгоритм

Двухшаговый МНК используется при сверхидентификации ($m > k$).

**Алгоритм оценивания:**

**Шаг 1:** Оценка приведенной формы - регрессия каждого эндогенного регрессора на все инструменты методом МНК:

$$\hat{X}_j = Z(Z'Z)^{-1}Z'X_j, \quad j = 1, \ldots, k$$

**Шаг 2:** Оценка исходного уравнения методом МНК, где эндогенные переменные заменены их предсказанными значениями из первого шага:

$$Y = \beta_0 + \beta_1\hat{X}_1 + \cdots + \beta_k\hat{X}_k + \varepsilon$$

### Математическая формула 2SLS

Оценка двухшагового МНК записывается как:

$$\hat{\beta}_{2SLS} = (X'P_Z X)^{-1}X'P_Z Y$$

где $P_Z = Z(Z'Z)^{-1}Z'$ - проекционная матрица.

**Важное замечание:** Распространенная ошибка - включение самих инструментов на втором шаге. Двухшаговый подход не обеспечивает эффективность оценок в этом случае.

## Условия идентификации

### Порядковое условие идентификации

**Необходимое условие:** Количество инструментов должно быть не меньше количества регрессоров:

$$m \geq k$$

**Словесная формулировка:** Количество инструментов $Z$ должно быть не меньше количества регрессоров $X$.

### Ранговое условие идентификации

**Необходимое и достаточное условие:** Матрица $\hat{X}$ имеет полный ранг по столбцам:

$$\text{rank } \hat{X} = k$$

### Типы идентификации

- **Точная идентификация:** $m = k$ (применяется метод IV)
- **Сверхидентификация:** $m > k$ (применяется 2SLS)
- **Недоидентификация:** $m < k$ (невозможно получить оценки)


## Проверка качества инструментов

### 1. Тест релевантности (проверка силы инструментов)

**Критерий Стока-Райта-Його:** F-статистика первого шага 2SLS должна быть больше 10. Если $F > 10$, то максимально возможное смещение 2МНК-оценки будет не слишком велико - в пределах 10% от истинного значения параметра.

**Более строгий современный критерий:** Lee et al. (2020) предлагают критерий F-статистики больше 104.7.

**Последствия слабых инструментов:** Слабые инструменты приводят к смещению оценок двухшагового МНК.

### 2. Тест валидности (тест Саргана)

**Условие применения:** Доступен только при $m > p$ (сверхидентификация).

**Гипотезы:**

- $H_0$: Все инструменты экзогенные
- $H_1$: Хотя бы один инструмент эндогенен

**Тестовая статистика при гомоскедастичности (тест Саргана):**

$$J = \frac{1}{\sigma_\varepsilon^2}\hat{\varepsilon}'Z(Z'Z)^{-1}Z'\hat{\varepsilon} \sim \chi^2_{m-k}$$

**Тестовая статистика при гетероскедастичности:**

$$J = \hat{\varepsilon}'Z(Z'\Omega_Z Z)^{-1}Z'\hat{\varepsilon} \sim \chi^2_{m-k}$$

где $\Omega_Z$ учитывает структуру гетероскедастичности.

### 3. Тест Хаусмана

**Цель:** Косвенное тестирование эндогенности регрессоров.

**Статистика теста:**

$$\eta = (\hat{\beta}_{IV} - \hat{\beta}_{OLS})'(V(\hat{\beta}_{IV}) - V(\hat{\beta}_{OLS}))^{-1}(\hat{\beta}_{IV} - \hat{\beta}_{OLS}) \sim \chi^2$$

**Ограничение:** При отклонении $H_0$ неясно, что является причиной - эндогенность регрессоров или невалидность инструментов.

### 4. Процедура Хаусмана-Дарбина-Ву

**Применение:** Альтернатива тесту Хаусмана при его невыполнимости.

**Алгоритм для единственного эндогенного регрессора $X_2$:**

1. Построение вспомогательной регрессии эндогенного регрессора на экзогенные регрессоры и инструменты с извлечением остатков:
$X_2 = X_1\alpha_1 + Z\gamma + v$
2. Включение остатков в основное уравнение:
$Y = X_1\beta_1 + X_2\beta_2 + \delta\hat{v} + \varepsilon$
3. Проверка значимости остатков $\hat{v}$

## Принципы выбора инструментов

### Общие рекомендации

- Выбирать не более 3-4 инструментов для одного эндогенного регрессора
- Использовать метод главных компонент для отбора инструментов
- Чем слабее инструменты, тем меньше их должно быть


### Фундаментальная проблема выбора

**Противоречивость требований:** Если $Z$ должны сильно коррелировать с $X$, а $X$ сильно коррелируют с $Y$, то $Z$ будут коррелировать с $Y$ и, следовательно, возможно с ошибкой.

**Дилемма:** Инструменты либо экзогенные, но слабые, либо сильные, но эндогенные.

## Практические примеры применения

### Пример 1: Влияние образования на доход (Card, 1995)

**Исследуемая проблема:** Оценка причинного влияния образования на доходы.

**Инструмент:** Расстояние от места жительства до ближайшего колледжа.

**Обоснование релевантности:** Близость к колледжу увеличивает вероятность продолжения обучения после школы.

**Обоснование экзогенности:** Расположение дома рядом с колледжем само по себе не делает индивида более талантливым.

### Пример 2: Влияние телевидения на выборы (Enikolopov et al., 2011)

**Контекст:** Анализ воздействия телеканала НТВ на результаты выборов в российскую Думу 1999 года.

**Инструмент:** Доступность телеканала НТВ в регионе (определяется силой сигнала передатчиков).

**Обоснование релевантности:** При плохом качестве сигнала данный телеканал вряд ли будут смотреть.

**Обоснование экзогенности:** Техническое покрытие определяется не политическими предпочтениями зрителей, а техническими возможностями.

### Пример 3: Институты и экономический рост (Acemoglu, Robinson, 2001)

**Исследуемая проблема:** Влияние качества институтов на экономический рост.

**Источники эндогенности:**

1. Пропущенные важные переменные
2. Двусторонняя причинно-следственная связь (богатые страны могут позволить себе лучшие институты)
3. Ошибки измерения качества институтов

**Инструмент:** Смертность поселенцев во время колонизации.

**Обоснование релевантности:** В колониях с высокой смертностью завоевателей не создавались институты защиты прав собственности.

**Обоснование экзогенности:** Никакие шоки современного экономического роста не влияют на смертность колонистов несколько веков назад.

## Сравнение различных моделей

| Модель | Детерминированность регрессоров | Нормальность ошибок | Гомоскедастичность |
| :-- | :-- | :-- | :-- |
| Классическая линейная | Требуется | Требуется | Требуется |
| Обобщенная линейная | Не требуется | Требуется для тестирования | Не требуется |
| Со стохастическими регрессорами | Не требуется | Не требуется | Не требуется |

## Практические рекомендации

### Ключевые принципы

Не существует автоматического алгоритма выбора инструментов. Поэтому требуется не только владение эконометрической техникой, но и хорошее содержательное понимание исследуемой проблемы.

### Методы устранения эндогенности

При наличии эндогенности необходимо применять следующие методы оценивания:

- Метод инструментальных переменных
- Двухшаговый метод наименьших квадратов
- Обобщенный метод моментов


## Заключение

Эндогенность представляет собой одну из наиболее серьезных проблем в эконометрическом анализе. Успешное решение проблемы эндогенности требует не только технического владения методами инструментальных переменных и двухшагового МНК, но и глубокого понимания исследуемых экономических процессов для правильного выбора и обоснования инструментов. Правильная идентификация причинно-следственных связей критически важна для получения надежных выводов в эмпирических исследованиях.


# Лекция 02. Эффекты воздействия

## Введение

Данная лекция посвящена методам оценки эффекта воздействия в эконометрике, которые позволяют установить причинно-следственные связи между переменными. Эти методы широко применяются для анализа последствий различных политик, программ и мероприятий в экономике, социологии, медицине и других областях. Лектор - Вакуленко Е.С., д.э.н., профессор департамента прикладной экономики.

## Оценка эффекта воздействия в идеальном эксперименте

### Основные определения

- **Группа воздействия (treatment group)** - группа объектов, которая подверглась воздействию.
- **Контрольная группа (control group)** - группа объектов, которая не подвергалась воздействию.
- **Эффект воздействия (treatment effect)** - изменение значения зависимой переменной в результате воздействия.


### Математическая формализация

Введем бинарную переменную $D_i$:

- $D_i = 1$, если i-й объект вошел в группу воздействия. Значение зависимой переменной в этом случае обозначим $Y_i(1)$.
- $D_i = 0$, если i-й объект вошел в контрольную группу. Значение зависимой переменной в этом случае обозначим $Y_i(0)$.

Эффект воздействия для i-го объекта определяется как:

$$Y_i(1) - Y_i(0)$$

Это изменение значения зависимой переменной в результате воздействия.

### Средний эффект воздействия (ATE)

Если эффект воздействия различается для разных объектов, то он называется гетерогенным. Средний эффект воздействия (average treatment effect, ATE) - это усреднение эффекта воздействия по всем объектам из генеральной совокупности.

### Проблема потенциальных исходов

Для расчета эффекта воздействия необходимо вычислить разность $Y_i(1) - Y_i(0)$, но на практике для каждого объекта мы наблюдаем только один из этих показателей. Эти два значения называются потенциальными исходами (potential outcomes).

Фактически наблюдаемое значение зависимой переменной:

$$
Y_i = \begin{cases}
Y_i(1), \text{ если } D_i = 1 \\
Y_i(0), \text{ если } D_i = 0
\end{cases}
$$

Или в альтернативной записи:

$$Y_i = Y_i(0) + D_i(Y_i(1) - Y_i(0))$$

### Расчет ATE

Для оценки эффекта воздействия можно сопоставить ожидаемые значения зависимой переменной в группе воздействия и контрольной группе:

$$E(Y_i|D_i = 1) - E(Y_i|D_i = 0)$$

Это выражение можно преобразовать:

$$E(Y_i|D_i = 1) - E(Y_i|D_i = 0) = E(Y_i(1)|D_i = 1) - E(Y_i(0)|D_i = 0) = $$
$$= E(Y_i(1)|D_i = 1) - E(Y_i(0)|D_i = 1) + E(Y_i(0)|D_i = 1) - E(Y_i(0)|D_i = 0) = $$
$$= E(Y_i(1) - Y_i(0)|D_i = 1) + E(Y_i(0)|D_i = 1) - E(Y_i(0)|D_i = 0) = ATET + \textit{selection bias}$$

Здесь ATET - средний эффект воздействия для объектов, которые подверглись воздействию (average treatment effect on the treated). Второе слагаемое называется смещением из-за самоотбора (selection bias).

### Случайное распределение по группам

Если объекты случайным образом распределяются между группой воздействия и контрольной группой (random assignment), то смещение из-за самоотбора равно нулю:

$$E(Y_i(0)|D_i = 1) = E(Y_i(0)|D_i = 0) = E(Y_i(0))$$

В этом случае разность условных математических ожиданий равна среднему эффекту воздействия:

$$E(Y_i|D_i = 1) - E(Y_i|D_i = 0) = ATET$$

Состоятельная оценка среднего эффекта воздействия:

$$\bar{Y}_1 - \bar{Y}_0 = ATET$$

где $\bar{Y}_1$ и $\bar{Y}_0$ - средние значения зависимой переменной в группе воздействия и контрольной группе соответственно.

## Оценка эффекта воздействия с помощью регрессии

Эффект воздействия можно оценить с помощью парной регрессии:

$$Y_i = \beta_1 + \beta_2 \cdot D_i$$

Оценка МНК:

$$\hat{\beta}_2 = \bar{Y}_1 - \bar{Y}_0$$

Если эксперимент построен корректно, то объясняющая переменная является экзогенной благодаря случайному распределению объектов по группам. Поэтому обычная парная регрессия дает несмещенную и состоятельную оценку среднего эффекта воздействия.

### Добавление контрольных переменных

Включение контрольных переменных в регрессию может быть полезно по двум причинам:

1. Увеличение точности оценивания: контрольные переменные позволяют лучше описать зависимую переменную и получить более точные оценки коэффициентов.
2. Проверка качества рандомизации: если эксперимент построен правильно, то оценки коэффициента при переменной воздействия в парной и множественной регрессии не должны сильно отличаться.

### Пример: эксперимент STAR

Проект STAR (Student/Teacher Achievement Ratio) проводился в США в 80-х годах с целью выяснить, помогает ли обучение в меньших группах достигать больших академических успехов. Школьники случайным образом распределялись по классам разного типа: стандартного размера (22-25 человек) и уменьшенного (13-17 человек).

Зависимая переменная - результаты стандартизированного письменного теста. Переменная интереса - бинарная переменная, равная единице для школьников, обучавшихся в маленьком классе.

Результаты показали, что эффект воздействия от попадания в маленький класс устойчив к выбору спецификации и статистически значим. Обучение в маленьком классе увеличивает результаты школьника на итоговом тесте примерно на 6 баллов.

## Метод разности разностей (Difference-in-Differences, DiD)

### Пример: влияние минимальной заработной платы на занятость

В 1992 году в штате Нью-Джерси минимальная заработная плата была увеличена с 4,25 до 5,05 долларов. Исследователи собрали данные о занятости работников в ресторанах быстрого питания. Зависимая переменная - число работников, занятых в ресторане полный рабочий день.

### Идея метода разности разностей

Идея метода заключается в сравнении средней занятости в ресторанах Нью-Джерси (группа воздействия) со средней занятостью в ресторанах другого штата, где минимальная зарплата не изменилась (контрольная группа). Однако штаты могут отличаться не только оцениваемыми мерами, но и другими характеристиками.

На занятость в ресторане могут влиять:

- Специфические особенности штата (эффект штата)
- Особенности различных периодов времени (временной эффект)
- Эффект изменения минимальной заработной платы (эффект воздействия)


### Математическая модель

$$Y_{ist} = \alpha_s + \mu_t + \delta \cdot D_{ist} + \varepsilon_{ist}$$

где:

- $i$ - номер ресторана
- $s$ - штат (Нью-Джерси или Пенсильвания)
- $t$ - момент времени (до или после изменения заработной платы)
- $Y_{ist}$ - число работников, занятых в ресторане
- $D_{ist}$ - бинарная переменная, равная единице для ресторанов в Нью-Джерси после изменения заработной платы
- $\alpha_s$ - эффект штата
- $\mu_t$ - временной эффект
- $\delta$ - эффект воздействия
- $\varepsilon_{ist}$ - случайные ошибки модели


### Расчет эффекта воздействия

Ожидаемое количество занятых в ресторане Нью-Джерси до изменения заработной платы:

$$E(Y_{ist}|s = treatment, t = before) = \mu_{before} + \alpha_{treatment}$$

Ожидаемое количество занятых в ресторане Нью-Джерси после изменения:

$$E(Y_{ist}|s = treatment, t = after) = \mu_{after} + \alpha_{treatment} + \delta$$

Ожидаемое изменение занятости в Нью-Джерси:

$$\Delta_{treatment} = \mu_{after} - \mu_{before} + \delta$$

Аналогично для Пенсильвании:

$$\Delta_{control} = \mu_{after} - \mu_{before}$$

Разность разностей:

$$\Delta_{treatment} - \Delta_{control} = \delta$$

Оценка эффекта воздействия:

$$\hat{\delta} = [\bar{Y}_{treatment,after} - \bar{Y}_{treatment,before}] - [\bar{Y}_{control,after} - \bar{Y}_{control,before}]$$

### Результаты исследования Card и Krueger (1994)

Повышение минимальной заработной платы привело к увеличению равновесного уровня занятости в ресторанах быстрого питания Нью-Джерси в среднем на 2,76 человека. Этот результат противоречит выводам стандартных теоретических моделей из микроэкономики.

### Оценка с помощью регрессии

Метод разности разностей можно реализовать с помощью регрессии:

$$Y_{it} = \beta_0 + \beta_1 \cdot X_i + \beta_2 \cdot Z_t + \delta \cdot X_i \cdot Z_t + \varepsilon_{it}$$

где:

- $X_i$ - бинарная переменная, равная единице, если i-й ресторан расположен в Нью-Джерси
- $Z_t$ - бинарная переменная, равная единице для наблюдений после повышения минимальной зарплаты
- $\delta$ - эффект воздействия

Оценки МНК:

- $\hat{\beta}_0 = \bar{Y}_{control,before}$
- $\hat{\beta}_1 = \bar{Y}_{treatment,before} - \bar{Y}_{control,before}$
- $\hat{\beta}_2 = \bar{Y}_{control,after} - \bar{Y}_{control,before}$
- $\hat{\delta} = [\bar{Y}_{treatment,after} - \bar{Y}_{treatment,before}] - [\bar{Y}_{control,after} - \bar{Y}_{control,before}]$


## Синтетическая контрольная группа

### Проблема выбора контрольной группы

Метод синтетической контрольной группы (Abadie, Diamond \& Hainmueller, 2007) позволяет создать искусственную контрольную группу как взвешенную комбинацию нескольких потенциальных контрольных групп.

### Математическая формализация

Пусть $Y_{it}$ - исход для объекта $i$ в периоде $t$, где $i = 1$ - объект, подверженный воздействию во втором периоде. Имеется $J$ возможных контрольных групп, индексируемых как $\{2, ..., J+1\}$. Эффект программы можно оценить как:

$$Y_{12} - \sum_{j=2}^{J+1} w_j Y_{j2}$$

где $w_j$ - неотрицательные веса, в сумме дающие единицу.

Abadie et al. (2007) предлагают выбирать веса, минимизирующие расстояние между $(Y_{11}, X_1)$ и $\sum_{j=2}^{J+1} w_j (Y_{j1}, X_j)$.

### Пример: эффекты кризиса 2014 года на рождаемость в России

Исследование влияния экономического кризиса 2014 года на рождаемость в России с использованием метода синтетической контрольной группы. Рассматривается годовой темп роста показателей рождаемости с 2011 по 2017 гг. в России (группа воздействия) в сравнении с данными по европейским странам, избежавшим кризиса 2014 г. (контрольная группа).

Результаты показали, что разница в темпах роста рождаемости, вызванная экономическим кризисом, составляет 6 процентных пунктов. Зависимость проциклическая: рождаемость падает спустя 1 квартал после начала рецессии, но затем возвращается на предыдущий уровень.

## Динамический метод разности разностей

### Проблема неодновременного воздействия

Стандартный метод разности разностей предполагает, что воздействие происходит одновременно для всех объектов в группе воздействия. Однако на практике воздействие может происходить в разное время для разных объектов.

### Динамический DiD (event study analysis)

$$Y_{i,t} = \alpha_i + \gamma_t + \sum_{j=2}^{J} \beta_j (Lag_j)_{i,t} + \sum_{k=1}^{K} \beta_k (Lead_k)_{i,t} + X_{i,t-1}\theta + \varepsilon_{i,t}$$

где:

- $(Lag_j)_{i,t} = 1$, если после момента наступления эффекта воздействия прошло $j$ периодов для объекта $i$ в момент времени $t$, иначе 0
- $(Lead_k)_{i,t} = 1$, если до момента наступления эффекта воздействия осталось $k$ периодов для объекта $i$ в момент времени $t$, иначе 0

Включение переменных Lead позволяет тестировать гипотезу о равенстве трендов в контрольной группе и группе воздействия до периода воздействия. Включение переменных Lag позволяет тестировать, меняется ли эффект воздействия со временем.

### Пример: оценка эффектов региональных программ материнского капитала

Исследование влияния региональных программ материнского капитала (РМК) на суммарный коэффициент рождаемости (СКР) по очередности рождений. Регионы РФ вводили программу РМК в разные годы, что делает необходимым использование динамического метода разности разностей.

Результаты показали различное влияние программы РМК на рождаемость первых и вторых детей.

## Локальный средний эффект воздействия (LATE)

### Эндогенное разделение на группы

В некоторых случаях разделение на группу воздействия и контрольную группу может быть эндогенным, например, если индивиды сами решают, подвергаться ли им воздействию. В этих условиях корректно оценить ATE затруднительно, но можно оценить локальный средний эффект воздействия (local average treatment effect, LATE).

### Пример: оценка влияния службы в армии на будущие доходы

Исследование Angrist (1990) о влиянии службы в армии на будущие доходы ветеранов войны во Вьетнаме. Приоритетность призыва зависела от случайного порядкового номера (Random Sequence Number, RSN), который присваивался каждому мужчине в результате розыгрыша "лотереи".

### Двустороннее несоблюдение

Статус "победителя лотереи" не равен статусу ветерана войны:

- Не все ветераны войны были "победителями лотереи" (некоторые записывались добровольно)
- Не все "победители лотереи" стали ветеранами (некоторые избежали службы по медицинским причинам)

Такая ситуация называется двусторонним несоблюдением (two-sided noncompliance).

### Математическая формализация

Пусть $Y_i$ - доход i-го индивида, $D_i$ - бинарная переменная, равная единице, если i-й индивид служил в армии, $Z_i$ - бинарная переменная, равная единице, если i-й индивид "выиграл в лотерее".

Переменная воздействия зависит от инструмента:

$$
D_i = D_i(Z_i) = \begin{cases}
D_i(1), \text{ если } Z_i = 1 \\
D_i(0), \text{ если } Z_i = 0
\end{cases}
$$

Зависимая переменная может зависеть и от переменной воздействия, и от инструмента:

$$
Y_i = Y_i(D_i, Z_i) = \begin{cases}
Y_i(0,0), \text{ если } D_i = 0, Z_i = 0 \\
Y_i(0,1), \text{ если } D_i = 0, Z_i = 1 \\
Y_i(1,0), \text{ если } D_i = 1, Z_i = 0 \\
Y_i(1,1), \text{ если } D_i = 1, Z_i = 1
\end{cases}
$$

### Оценка LATE

При выполнении определенных предпосылок:

$$LATE = \frac{E(Y_i|Z_i = 1) - E(Y_i|Z_i = 0)}{E(D_i|Z_i = 1) - E(D_i|Z_i = 0)} = \frac{\bar{Y}_1 - \bar{Y}_0}{\bar{D}_1 - \bar{D}_0}$$

где:

- $\bar{Y}_1$ - средний доход тех, кто "выиграл в лотерею"
- $\bar{Y}_0$ - средний доход тех, кто "проиграл в лотерею"
- $\bar{D}_1$ - доля "победителей лотереи", которые пошли служить
- $\bar{D}_0$ - доля "проигравших в лотерею", которые при этом всё равно пошли служить

В случае, когда инструментальная переменная и эндогенный регрессор являются бинарными, 2МНК-оценка совпадает с оценкой LATE.

## Разрывный регрессионный дизайн (Regression Discontinuity Design, RDD)

### Идея метода

В некоторых квазиэкспериментах вероятность того, что объект подвергнется воздействию, является разрывной функцией от наблюдаемой переменной. В этом случае для оценки эффекта воздействия может быть применен разрывный регрессионный дизайн.

### Виды разрывного дизайна

- Четкий (sharp) - вероятность воздействия меняется от 0 до 1 при преодолении порогового значения
- Нечеткий (fuzzy) - вероятность воздействия меняется при преодолении порогового значения, но не от 0 до 1


### Четкий разрывный дизайн

Пример: оценка влияния обучения в магистратуре университета на будущий доход. Поступление в вуз зависит от проходного балла.

Математическая модель:

$$Y_i = \alpha + \beta x_i + \rho D_i + \varepsilon_i$$

$$
D_i = \begin{cases}
0, \text{ при } x_i < x^* \\
1, \text{ при } x_i \geq x^*
\end{cases}
$$

где $x^*$ - пороговое значение (проходной балл).

Условные математические ожидания:

- Для индивида из контрольной группы: $E(Y_i(0)|x_i) = \alpha + \beta x_i$
- Для индивида из группы воздействия: $E(Y_i(1)|x_i) = \alpha + \beta x_i + \rho$


### Нелинейный случай RDD

Влияние переменной отбора на зависимую переменную может быть нелинейным:

$$Y_i = \alpha + f(x_i) + \rho D_i + \varepsilon_i$$

$$
D_i = \begin{cases}
0, \text{ при } x_i < x^* \\
1, \text{ при } x_i \geq x^*
\end{cases}
$$

В качестве функции $f(x_i)$ обычно используют полином.

### Нечеткий разрывный дизайн

$$
P(D_i = 1|x_i) = \begin{cases}
g_0(x_i), \text{ при } x_i < x^* \\
g_1(x_i), \text{ при } x_i \geq x^*
\end{cases}
$$

где $g_0(x^*) \neq g_1(x^*)$.

При нечетком разрывном дизайне переменная $D$ в уравнении $Y_i = \alpha + f(x_i) + \rho D_i + \varepsilon_i$ не является экзогенной. Поэтому для оценки эффекта воздействия применяют 2МНК, используя в качестве инструмента переменную:

$$
T_i = \begin{cases}
0, \text{ при } x_i < x^* \\
1, \text{ при } x_i \geq x^*
\end{cases}
$$

### Пример: партия власти и результаты выборов

Исследование Lee (2008) о преимуществе партии власти на выборах. Результаты показали, что партия власти получает существенное преимущество на выборах независимо от текущих предпочтений избирателей.

### Тест плацебо

Тест плацебо (placebo test) - подход к проверке надежности результатов, который заключается в анализе спецификации модели, похожей на базовую, но в которой точно не должно возникать значимого эффекта воздействия. Если эффект действительно не возникает, это говорит в пользу корректности выводов базовой модели.

## Методы машинного обучения для идентификации эффектов воздействия

### Использование ML для оценки эффектов воздействия

Методы машинного обучения могут использоваться для предсказания ненаблюдаемого слагаемого при оценке эффекта воздействия:

$$E(Y_i | X_i = 1, W_i) - E(Y_i | X_i = 0, W_i)$$

где $X \in \{0, 1\}$ - переменная воздействия, $W$ - набор контрольных переменных.

Можно использовать различные ансамблевые методы: обобщенную линейную регрессию, регрессию Lasso, регрессию Ridge, случайный лес, метод опорных векторов и метод честных деревьев (Honest Trees).

### Пример: влияние пандемии COVID-19 на намерения родить детей в России

Исследование показало, что эффект воздействия коронавирусных ограничений статистически значим только для краткосрочных намерений родить ребенка (в ближайшие 3 года). Оценка коэффициента равна 0.061, стандартная ошибка 0.021.

## А/В-тестирование

### Суть А/В-тестирования

А/В-тестирование - простой рандомизированный контролируемый эксперимент, в котором сравнивается несколько выборок (А и В). Это маркетинговый инструмент, суть которого заключается в сравнении контрольной группы элементов с набором тестовых групп, в которых один или несколько показателей были изменены.

### Примеры применения А/В-тестирования

- Веб-дизайн (размер, цвет, форма кнопок, шрифта и т.д.)
- Обращение к пользователям при email-рассылках
- Расстановка товаров на полках в магазинах
- Цена товара


### Процедура А/В-теста

1. Определить цель проведения теста
2. Определить метрику
3. Определить основную и альтернативную гипотезы
4. Подготовить эксперимент
5. Провести эксперимент
6. Проверить гипотезу с помощью статистического теста

### Сегментация и таргетинг

А/В-тесты чаще всего применяют один и тот же вариант с равной вероятностью для всех пользователей. Однако в некоторых случаях ответы на варианты могут быть неоднородными. То есть, в то время как вариант А может иметь более высокую частоту откликов в целом, вариант В может иметь еще более высокую частоту откликов в пределах определенного сегмента клиентской базы.

### Статистики для тестирования гипотез

Для тестирования гипотез в А/В-тестировании могут использоваться различные статистические тесты в зависимости от предполагаемого распределения:

- Нормальное распределение: t-тест Уэлча
- Биномиальное распределение: точный тест Фишера
- Неизвестное распределение: U-тест Манна-Уитни или бутстрэп

Но проще всего оценить регрессию с дамми-переменной на тип группы:

$$Y_i = \beta_1 + \beta_2 \cdot D_i$$

Проверка значимости коэффициента при дамми-переменной $D$ и есть проверка интересующей нас гипотезы.

## Заключение

Методы оценки эффекта воздействия играют важную роль в эконометрическом анализе, позволяя установить причинно-следственные связи между переменными. Каждый из рассмотренных методов имеет свои преимущества и ограничения, и выбор конкретного метода зависит от особенностей исследуемой проблемы и доступных данных.


# Лекция 03. Системы одновременных уравнений

## Введение в системы одновременных уравнений

Системы одновременных уравнений представляют собой фундаментальный инструмент эконометрического анализа, позволяющий моделировать взаимосвязи между несколькими экономическими переменными. В отличие от простых регрессионных моделей, где одна переменная объясняется другими, системы одновременных уравнений учитывают взаимное влияние переменных друг на друга.

### Базовый пример: простая кейнсианская модель

Рассмотрим классический пример простой кейнсианской модели:

$$C_t = \beta_1 + \beta_2 Y_t + \varepsilon_t$$
$$Y_t = C_t + I_t$$

где:

- $C_t$ — потребление в период $t$
- $Y_t$ — доход в период $t$
- $I_t$ — инвестиции в период $t$
- $\beta_2$ — предельная склонность к потреблению


## Формальное представление системы уравнений

### Система на уровне уравнений

Общая система из $M$ уравнений записывается как:

$$y_1 = X_1\beta_1 + \varepsilon_1$$
$$y_2 = X_2\beta_2 + \varepsilon_2$$
$$\vdots$$
$$y_M = X_M\beta_M + \varepsilon_M$$

где:

- $y_i$ — вектор зависимых переменных размером $n \times 1$
- $X_i$ — матрица регрессоров размером $n \times k_i$
- $\beta_i$ — вектор параметров размером $k_i \times 1$
- $\varepsilon_i$ — вектор ошибок размером $n \times 1$


### Матричная форма записи

Система может быть записана в компактной матричной форме:

$$y = X\beta + \varepsilon$$

где:

$$y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_M \end{pmatrix}

\quad X = \begin{pmatrix} X_1 & 0 & \cdots & 0 \\ 0 & X_2 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & X_M \end{pmatrix} 

\quad \beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_M \end{pmatrix}$$

## Предпосылки модели

Для корректного оценивания системы одновременных уравнений необходимо выполнение следующих условий:

1. **Нулевое математическое ожидание ошибок**: $E(\varepsilon_i) = 0$
2. **Структура ковариационной матрицы ошибок**: $E[\varepsilon_i \varepsilon_j'] = \sigma_{ij}I_n$ при $i,j = 1,...,M$
3. **Общая ковариационная матрица**:
$E[\varepsilon\varepsilon'] = \Omega = \Sigma \otimes I_n$

где $\Sigma = [\sigma_{ij}]$ — матрица ковариаций ошибок между уравнениями.

## Обобщенный метод наименьших квадратов (GLS)

### Общая формула оценивания

При наличии корреляции между ошибками разных уравнений применяется обобщенный МНК:

$$\hat{\beta}_{GLS} = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y$$

Используя свойство произведения Кронекера $(A \otimes B)^{-1} = A^{-1} \otimes B^{-1}$, получаем:

$$\hat{\beta}_{GLS} = (X'(\Sigma^{-1} \otimes I_n)X)^{-1}X'(\Sigma^{-1} \otimes I_n)y$$

### Специальные случаи

**Случай 1**: Отсутствие корреляции между уравнениями ($\sigma_{ij} = 0$ при $i \neq j$)

**Случай 2**: Одинаковые матрицы регрессоров ($X_1 = X_2 = \cdots = X_M$)

В этих случаях обобщенный МНК сводится к применению обычного МНК к каждому уравнению отдельно.

### Оценивание ковариационной матрицы

На практике матрица $\Sigma$ неизвестна и оценивается через остатки:

$$s_{ij} = \frac{e_i'e_j}{n}$$

где $e_i$ — вектор остатков $i$-го уравнения, полученных методом МНК.

## Системы спроса и предложения

### Классическая модель рынка

Рассмотрим систему уравнений спроса и предложения:

**Уравнение предложения**:
$Q_t^S = \alpha_1 + \alpha_2 P_t + \varepsilon_t$

**Уравнение спроса**:
$Q_t^D = \beta_1 + \beta_2 P_t + \beta_3 Y_t + u_t$

**Условие равновесия**:
$Q_t^S = Q_t^D = Q_t$

### Проблема идентификации

При равновесии наблюдаются только цена $p_t$ и количество $q_t$, что создает проблему идентификации параметров. В приведенной форме система записывается как:

$$q_t = \pi_1 y_t + v_{1t}$$
$$p_t = \pi_2 y_t + v_{2t}$$

где:

- $\pi_1 = \frac{\alpha_2\beta_3}{\alpha_2 - \beta_2}$
- $\pi_2 = \frac{\beta_3}{\alpha_2 - \beta_2}$


### Косвенный метод наименьших квадратов (ILS)

Для восстановления структурных параметров используется соотношение:

$$\hat{\alpha}_2 = \frac{\hat{\pi}_1}{\hat{\pi}_2}$$

где $\hat{\pi}_1$ и $\hat{\pi}_2$ — оценки МНК приведенной формы.

## Структурная форма модели

### Общая запись структурной формы

Система $m$ структурных уравнений с $m$ эндогенными и $k$ экзогенными переменными записывается как:

$$\mathbf{B}y_t + \mathbf{\Gamma}x_t = \varepsilon_t$$

где:

- $\mathbf{B}$ — матрица коэффициентов при эндогенных переменных ($m \times m$)
- $\mathbf{\Gamma}$ — матрица коэффициентов при экзогенных переменных ($m \times k$)
- Матрица $\mathbf{B}$ невырожденная


### Приведенная форма

Из структурной формы получается приведенная форма:

$$y_t = -\mathbf{B}^{-1}\mathbf{\Gamma}x_t + \mathbf{B}^{-1}\varepsilon_t = \Pi x_t + v_t$$

где $\Pi = -\mathbf{B}^{-1}\mathbf{\Gamma}$ — матрица коэффициентов приведенной формы.

## Условия идентификации

### Порядковое условие идентификации

Для первого уравнения системы необходимое условие идентификации:

$$k - p \geq q - 1$$

где:

- $k$ — общее число экзогенных переменных в системе
- $p$ — число экзогенных переменных в данном уравнении
- $q$ — число эндогенных переменных в данном уравнении


### Ранговое условие идентификации

Необходимое и достаточное условие идентификации:

$$\text{rank}(\boldsymbol{\Pi}_{*,\times\times}) = q - 1$$

где $\boldsymbol{\Pi}_{*,\times\times}$ — подматрица коэффициентов приведенной формы для исключенных из уравнения переменных.

## Двухшаговый метод наименьших квадратов (2SLS)

### Алгоритм оценивания

Для оценивания структурного уравнения вида:

$$y_1 = Y_1\beta_1 + X_1\gamma_1 + \varepsilon_1$$

**Шаг 1**: Оценка приведенной формы для эндогенных регрессоров:
$Y_1 = X\Pi_1 + V$

$$\hat{\Pi}_1 = (X'X)^{-1}X'Y_1$$

**Шаг 2**: Регрессия зависимой переменной на предсказанные значения:
$$y_1 = \hat{Y}_1\beta_1 + X_1\gamma_1 + \varepsilon_1$$

где $\hat{Y}_1 = X\hat{\Pi}_1$ — предсказанные значения эндогенных регрессоров.

## Практические примеры

### Модель Клейна

Рассмотрим упрощенную макроэкономическую модель:

**Потребление**:
$C_t = \alpha_0 + \alpha_1 P_t + \alpha_2 P_{t-1} + \alpha_3 (W_t^p + W_t^g) + \varepsilon_{1t}$

**Инвестиции**:
$I_t = \beta_0 + \beta_1 P_t + \beta_2 P_{t-1} + \beta_3 K_{t-1} + \varepsilon_{2t}$

**Заработная плата в частном секторе**:
$W_t^p = \gamma_0 + \gamma_1 X_t + \gamma_2 X_{t-1} + \gamma_3 A_t + \varepsilon_{3t}$

**Тождества**:

- $X_t = C_t + I_t + G_t$ (совокупный спрос)
- $P_t = X_t - T_t - W_t^p$ (доход частного сектора)
- $K_t = K_{t-1} + I_t$ (накопление капитала)


### Модель предложения труда

Система одновременных уравнений для анализа предложения труда:

$$\text{hours} = \gamma_{12}\log(\text{wage}) + \delta_{10} + \delta_{11}\text{educ} + \delta_{12}\text{age} + \delta_{13}\text{kidslt6} + \delta_{14}\text{kidsge6} + \delta_{15}\text{nwifeinc} + u_1$$

$$\log(\text{wage}) = \gamma_{21}\text{hours} + \delta_{20} + \delta_{21}\text{educ} + \delta_{22}\text{exper} + \delta_{23}\text{exper}^2 + u_2$$

## Трехшаговый метод наименьших квадратов (3SLS)

### Формулировка проблемы

При наличии корреляции ошибок между уравнениями используется трехшаговый МНК:

$$y = ZB + \varepsilon$$

где $Z$ — блочно-диагональная матрица регрессоров.

### Алгоритм оценивания

**Шаг 1**: Оценка каждого уравнения методом 2SLS

**Шаг 2**: Оценка ковариационной матрицы ошибок по остаткам 2SLS

**Шаг 3**: Применение обобщенного МНК:

$$\hat{B} = (\hat{Z}'(\hat{\Sigma}^{-1} \otimes I)\hat{Z})^{-1}\hat{Z}'(\hat{\Sigma}^{-1} \otimes I)y$$

где $\hat{Z}$ — матрица инструментальных переменных, полученная на первом шаге.

## Заключение

Системы одновременных уравнений представляют собой мощный инструмент для моделирования сложных экономических взаимосвязей. Ключевыми аспектами успешного применения являются правильная идентификация модели, выбор подходящих инструментальных переменных и учет структуры ковариационной матрицы ошибок. Методы 2SLS и 3SLS обеспечивают состоятельные оценки параметров при выполнении соответствующих условий идентификации.
]

# Лекция 04. Метод максимального правдоподобия

## Введение

Метод максимального правдоподобия (Maximum Likelihood, ML) является одним из фундаментальных методов статистического оценивания параметров в эконометрике. В отличие от метода наименьших квадратов, который минимизирует сумму квадратов остатков, ML-метод максимизирует вероятность наблюдения имеющихся данных при заданных значениях параметров модели.


## Математический аппарат метода максимального правдоподобия

### Основные определения

Рассмотрим последовательность случайных величин $\{Y_1, Y_2, \ldots\}$.

Пусть $h_n(y, \theta_0)$ - совместная плотность распределения случайных величин $y$, где вид функции известен, кроме параметра $\theta_0$.

**Функция правдоподобия (likelihood function):**
$L_n(\theta) = L_n(\theta, y) = h_n(y, \theta)$

**Оценка максимального правдоподобия** определяется как:
$L_n(\hat{\theta}_n(y), y) = \sup_{\theta \in \Theta} L_n(\theta, y)$

где $\hat{\theta}_n(y)$ называется оценкой максимального правдоподобия параметра $\theta_0$.

### Информационная матрица

В случае, когда $L_n(\theta)$ дважды дифференцируема по $\theta$, **гессиан (Hessian matrix)** определяется как:

$$H_n(\theta) = \frac{\partial^2 \ln L_n(\theta)}{\partial \theta \partial \theta'}$$

**Информационная матрица** в точке $\theta_0$ равна:
$F_n(\theta_0) = -E(H_n(\theta_0))$

## Свойства оценок максимального правдоподобия

ML-оценки обладают рядом важных асимптотических свойств:

### 1. Инвариантность

Пусть $\hat{\theta}$ - оценка максимального правдоподобия параметра $\theta$ и $g(\theta)$ - непрерывная функция. Тогда $g(\hat{\theta})$ является оценкой ML параметра $g(\theta)$.

### 2. Состоятельность

$$\text{plim } \hat{\theta} = \theta$$

### 3. Асимптотическая нормальность

$$\sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N(0, F^{-1}(\theta))$$

где $F(\theta)$ обозначает асимптотическую информационную матрицу.

Матрица ковариаций собственно оценки $\hat{\theta}$ равна:
$\text{Var}(\hat{\theta}) = \frac{1}{n}F^{-1}(\theta)$

### 4. Асимптотическая эффективность

Для любой другой состоятельной и асимптотически нормальной оценки $\tilde{\theta}$ выполнено:
$$\text{Var}(\hat{\theta}) \leq \text{Var}(\tilde{\theta})$$

## Оценка ML для линейной регрессии

### Модель и предпосылки

Рассмотрим стандартную линейную регрессионную модель:
$$y = X\beta + u$$

где:

- $u \sim N(0, \sigma^2 I_n)$ - вектор случайных ошибок
- Следовательно, $y \sim N(X\beta, \sigma^2 I_n)$


### Функция правдоподобия

Плотность распределения случайного вектора $y$ равна:
$$h(y) = (2\pi\sigma^2)^{-n/2} \exp\left(-\frac{(y - X\beta)'(y - X\beta)}{2\sigma^2}\right)$$

**Логарифмическая функция правдоподобия:**
$$\ln L(\beta, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}(y - X\beta)'(y - X\beta)$$

### Условия первого порядка

Частные производные логарифмической функции правдоподобия:

$$\frac{\partial \ln L}{\partial \beta'} = \frac{1}{\sigma^2}(y - X\beta)'X$$

$$\frac{\partial \ln L}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{(y - X\beta)'(y - X\beta)}{2\sigma^4}$$

### ML-оценки параметров

Приравнивая к нулю первые производные, находим оценки ML:

$$\hat{\beta} = (X'X)^{-1}X'y$$

$$\hat{\sigma}^2 = \frac{e'e}{n}$$

где $e = y - X\hat{\beta}$ - остатки модели.

## Сравнение оценок МНК и ML

### Коэффициенты регрессии

**Важный результат:** Оценки коэффициентов модели МНК и ML в точности совпадают:
$\hat{\beta}_{ML} = \hat{\beta}_{OLS} = (X'X)^{-1}X'y$

### Оценки дисперсии

Оценки дисперсии случайной составляющей различны:

- **МНК:** $s^2 = \frac{e'e}{n-k}$ (несмещенная оценка)
- **ML:** $\hat{\sigma}^2 = \frac{e'e}{n}$ (смещенная, но состоятельная оценка)

где $k$ - количество параметров в модели.

## Информационная матрица для линейной регрессии

### Частные производные второго порядка

$$\frac{\partial^2 \ln L}{\partial \beta \partial \beta'} = -\frac{X'X}{\sigma^2}$$

$$\frac{\partial^2 \ln L}{\partial \sigma^2 \partial \sigma^2} = \frac{n}{2\sigma^4} - \frac{(y - X\beta)'(y - X\beta)}{\sigma^6}$$

$$\frac{\partial^2 \ln L}{\partial \sigma^2 \partial \beta'} = -\frac{(y - X\beta)'X}{\sigma^4}$$

### Информационная матрица

Взяв математическое ожидание с обратным знаком:

$$-E\left[\frac{\partial^2 \ln L}{\partial \beta \partial \beta'}\right] = \frac{X'X}{\sigma^2}$$

$$-E\left[\frac{\partial^2 \ln L}{\partial \sigma^2 \partial \sigma^2}\right] = \frac{n}{2\sigma^4}$$

поскольку $E((y - X\beta)'(y - X\beta)) = n\sigma^2$.

Математическое ожидание смешанной производной равно нулю: $$E(y - X\beta) = 0$$

$$
F_n(\beta, \sigma^2) = \begin{pmatrix}
\frac{1}{\sigma^2}X'X & 0 \\
0 & \frac{n}{2\sigma^4}
\end{pmatrix}
$$

### Асимптотическая информационная матрица

$$
F(\beta, \sigma^2) = \lim_{n \to \infty} \frac{1}{n}F_n = \begin{pmatrix}
\frac{1}{\sigma^2}Q & 0 \\
0 & \frac{1}{2\sigma^4}
\end{pmatrix}
$$

где $Q = \lim_{n \to \infty} \frac{1}{n}X'X$.

### Обращенная информационная матрица

$$
F^{-1} = \begin{pmatrix}
\sigma^2 Q^{-1} & 0 \\
0 & 2\sigma^4
\end{pmatrix}
$$

**Практическое применение:** На практике матрица ковариаций аппроксимируется как $\hat{\sigma}^2(X'X)^{-1}$.

**Важное следствие:** Поскольку внедиагональные блоки матрицы $F$ равны нулю, оценки $\hat{\beta}$ и $\hat{\sigma}^2$ асимптотически независимы.

## Обобщенная линейная модель

### Модель с известной ковариационной матрицей

Рассмотрим обобщенную линейную модель:
$$y = X\beta + u$$

где $\text{Var}(u) = \Omega$ - известная положительно определенная симметричная матрица.

### Логарифмическая функция правдоподобия

$$\ln L(\beta) = \text{const} - \frac{1}{2}\ln|\Omega| - \frac{1}{2}(y - X\beta)'\Omega^{-1}(y - X\beta)$$

### Условия первого порядка

$$\frac{\partial \ln L(\beta)}{\partial \beta} = X'\Omega^{-1}(y - X\beta)$$

### Информационная матрица

$$F(\beta) = -E\left[\frac{\partial^2 \ln L(\beta)}{\partial \beta \partial \beta'}\right] = X'\Omega^{-1}X$$

### ML-оценка

$$\hat{\beta} = (X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y$$

Это совпадает с оценкой обобщенного МНК (GLS).

## Тестирование гипотез в рамках ML

### Общая постановка

Рассматривается гипотеза в виде системы $q$ ($q < k$) независимых линейных ограничений:
$$R\beta = r$$

где:

- $R$ - известная $q \times k$ матрица ранга $q$
- $r$ - известный $q \times 1$ вектор


### ML-оценки с ограничениями

**Функция Лагранжа:**
$$\mathcal{L}(\beta) = \ln L(\beta) - \lambda'(R\beta - r)$$

где $\lambda$ - вектор $q$ множителей Лагранжа.

**Условия экстремума:**
$$\frac{\partial \mathcal{L}(\beta)}{\partial \beta} = 0 \quad \text{и} \quad R\beta = r$$

**Система уравнений:**
$$X'\Omega^{-1}(y - X\beta) - R'\lambda = 0$$
$$R\beta = r$$

### Решение системы

Обозначим через $\tilde{\beta}$ и $\tilde{\lambda}$ решение системы:

$$\tilde{\beta} = \hat{\beta} - (X'\Omega^{-1}X)^{-1}R'\tilde{\lambda}$$

$$\tilde{\lambda} = (R(X'\Omega^{-1}X)^{-1}R')^{-1}(R\hat{\beta} - r)$$

**Окончательная формула:**
$$\tilde{\beta} = \hat{\beta} - (X'\Omega^{-1}X)^{-1}R'(R(X'\Omega^{-1}X)^{-1}R')^{-1}(R\hat{\beta} - r)$$

## Три основных теста для проверки гипотез

### 1. Тест Вальда (Wald test)

**Идея:** При выполнении нулевой гипотезы вектор $R\hat{\beta}$ должен быть близок к $r$.

**Распределение оценки:**
$R\hat{\beta} - r \sim N(R\beta - r, R(X'\Omega^{-1}X)^{-1}R')$

**Статистика теста:**
$W = (R\hat{\beta} - r)'(R(X'\Omega^{-1}X)^{-1}R')^{-1}(R\hat{\beta} - r) \sim \chi^2(q)$

**Особенность:** Тест Вальда использует только оценки модели без ограничения на параметры.

### 2. Тест множителей Лагранжа (LM test)

**Идея:** При выполнении нулевой гипотезы все множители Лагранжа должны быть равны нулю, поэтому вектор $\tilde{\lambda}$ должен быть близок к нулю.

**Распределение множителей:**
$\tilde{\lambda} \sim N(0, (R(X'\Omega^{-1}X)^{-1}R')^{-1})$

**Статистика теста:**
$LM = \tilde{\lambda}'R(X'\Omega^{-1}X)^{-1}R'\tilde{\lambda} \sim \chi^2(q)$

**Альтернативная форма:**
$LM = \tilde{u}'\Omega^{-1}X(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}\tilde{u}$

где $\tilde{u} = y - X\tilde{\beta}$.

**Особенность:** Тест LM использует только оценки с ограничением на параметры.

### 3. Тест отношения правдоподобия (LR test)

**Идея:** Если ограничение справедливо, то отношение максимальных значений функций правдоподобия для регрессии с ограничением и без ограничений должно быть близко к 1.

**Статистика теста:**
$LR = -2(\ln L(\tilde{\beta}) - \ln L(\hat{\beta})) = \tilde{u}'\Omega^{-1}\tilde{u} - \hat{u}'\Omega^{-1}\hat{u}$

При выполнении нулевой гипотезы $LR \sim \chi^2(q)$.

### Альтернативная форма LR-статистики

Поскольку $X\tilde{\beta} = X\hat{\beta} - X(\hat{\beta} - \tilde{\beta})$, то $\tilde{u} = \hat{u} + X(\hat{\beta} - \tilde{\beta})$.

После алгебраических преобразований:
$$\tilde{u}'\Omega^{-1}\tilde{u} = \hat{u}'\Omega^{-1}\hat{u} + (\hat{\beta} - \tilde{\beta})'X'\Omega^{-1}X(\hat{\beta} - \tilde{\beta})$$

Используя $\hat{\beta} - \tilde{\beta} = (X'\Omega^{-1}X)^{-1}R'\tilde{\lambda}$:

$$LR = \tilde{u}'\Omega^{-1}\tilde{u} - \hat{u}'\Omega^{-1}\hat{u} = \tilde{\lambda}'R(X'\Omega^{-1}X)^{-1}R'\tilde{\lambda} \sim \chi^2(q)$$

## Сравнение тестов при неизвестной Ω

### Случай известной Ω

Если $\Omega$ известна, то все три теста эквивалентны:
$LM = LR = W$

### Случай неизвестной Ω с известной структурой

Если $\Omega$ неизвестна, но известна ее структура $\Omega = \Omega(\theta)$, то:

$$W = (R\hat{\beta} - r)'(R(\hat{X}'\hat{\Omega}^{-1}\hat{X})^{-1}R')^{-1}(R\hat{\beta} - r)$$

$$LM = \tilde{u}'\tilde{\Omega}^{-1}X(X'\tilde{\Omega}^{-1}X)^{-1}X'\tilde{\Omega}^{-1}\tilde{u}$$

$$LR = -2(\ln L(\tilde{\beta}, \tilde{\theta}) - \ln L(\hat{\beta}, \hat{\theta}))$$

где $\hat{\Omega} = \Omega(\hat{\theta})$ и $\tilde{\Omega} = \Omega(\tilde{\theta})$.

**Соотношение между тестами:**
$$LM < LR < W$$

## Нелинейные ограничения

### Постановка задачи

Предположим, что нулевая гипотеза состоит из системы $q$ нелинейных ограничений на вектор коэффициентов $\beta$:
$$g(\beta) = 0$$

где $g(\cdot)$ - векторная функция размерности $q \times 1$.

### Модификация тестов

Пусть $G(\beta) = \frac{\partial g(\beta)}{\partial \beta'}$ - матрица Якоби размером $q \times k$.

**Тест Вальда:**
$$W = g(\hat{\beta})'(G(\hat{\beta})(X'\hat{\Omega}^{-1}X)^{-1}G(\hat{\beta})')^{-1}g(\hat{\beta})$$

**Тест LM:**
$$LM = \tilde{u}'\tilde{\Omega}^{-1}X(X'\tilde{\Omega}^{-1}X)^{-1}X'\tilde{\Omega}^{-1}\tilde{u}$$

**Тест LR:**
$$LR = -2(\ln L(\tilde{\beta}, \tilde{\theta}) - \ln L(\hat{\beta}, \hat{\theta}))$$

Все три статистики асимптотически имеют распределение $\chi^2(q)$.

## Проблемы со статистикой Вальда

### Проблема неинвариантности

Существует множество способов записи одного и того же ограничения, что может приводить к различным значениям статистики $W$.

**Пример:** Модель $y = \beta_1 x_1 + \beta_2 x_2 + u$, где $u \sim N(0, \sigma^2 I)$

Гипотеза: $\beta_1 = \beta_2$

**Способ 1:** $g_1(\beta) = \beta_1 - \beta_2$
$G_1(\beta) = (1, -1)$

**Способ 2:** $g_2(\beta) = \frac{\beta_1}{\beta_2} - 1$
$G_2(\beta) = \left(\frac{1}{\beta_2}, -\frac{\beta_1}{\beta_2^2}\right)$

Критические статистики Вальда имеют различный вид:
$$W_1 = \frac{(\hat{\beta}_1 - \hat{\beta}_2)^2}{\hat{\sigma}^2 d_1'(X'X)^{-1}d_1}$$

$$W_2 = \frac{(\hat{\beta}_1 - \hat{\beta}_2)^2}{\hat{\sigma}^2 d_2'(X'X)^{-1}d_2}$$

где $d_1 = (1, -1)$ и $d_2 = (1, -(\hat{\beta}_1/\hat{\beta}_2))$.

**Важное замечание:** Статистика Вальда неинвариантна по отношению к преобразованиям в нулевой гипотезе. Эти различия асимптотически исчезают, однако могут быть существенны в конечных выборках.

## Геометрическая интерпретация тестов

Три критерия можно интерпретировать графически на основе логарифмической функции правдоподобия:

- **Тест отношения правдоподобия (LR):** Сравнивает значения функции правдоподобия в точках максимума с ограничениями и без ограничений
- **Тест множителей Лагранжа (LM):** Основан на значении производной функции правдоподобия в точке ограниченного максимума
- **Тест Вальда (W):** Проверяет, насколько далеко находится неограниченная оценка от области ограничений


## Практические рекомендации

### Выбор теста

1. **Тест LR** - наиболее универсален, но требует оценивания как ограниченной, так и неограниченной модели
2. **Тест LM** - удобен, когда легко оценить ограниченную модель, но сложно - неограниченную
3. **Тест Вальда** - удобен, когда легко оценить неограниченную модель, но проблематично - ограниченную

### Общие соотношения

При малых выборках обычно выполняется:
$LM \leq LR \leq W$

При увеличении размера выборки все три теста становятся эквивалентными.

## Заключение

Метод максимального правдоподобия представляет собой мощный и универсальный подход к оцениванию параметров эконометрических моделей. Основные преимущества ML-метода включают оптимальные асимптотические свойства оценок, инвариантность к преобразованиям параметров и возможность построения эффективных тестов гипотез.

Три основных теста (Вальда, отношения правдоподобия и множителей Лагранжа) предоставляют исследователю гибкие инструменты для проверки различных типов ограничений на параметры модели. Выбор конкретного теста зависит от вычислительных особенностей задачи и свойств имеющихся данных.

# Лекция 05. Модели бинарного выбора

## Введение в модели бинарного выбора

Модели бинарного выбора представляют собой важный класс эконометрических моделей, где зависимая переменная принимает только два значения: 0 или 1. Эти модели широко применяются в различных областях экономического анализа для изучения решений типа "да/нет".


## Практические примеры применения бинарных моделей

Модели бинарного выбора находят широкое применение в решении следующих задач:

- **Макроэкономическое прогнозирование**: Оценка вероятности наступления рецессии в экономике и выявление факторов, влияющих на эту вероятность
- **Образовательная экономика**: Анализ решений выпускников школы о поступлении в высшие учебные заведения
- **Банковское дело**: Оценка кредитоспособности заемщиков и прогнозирование возврата долга


## Формальное определение бинарной зависимой переменной

Зависимая переменная в моделях бинарного выбора представляет собой индикатор события A:

$$
Y = I\{A\} = \begin{cases}
1, & \text{если событие } A \text{ произошло} \\
0, & \text{если событие } A \text{ не произошло}
\end{cases}
$$

## Свойства индикаторов событий

### Математическое ожидание

Фундаментальное свойство индикаторной переменной заключается в том, что ее математическое ожидание равно вероятности наступления события:

$$E(Y) = E(I\{A\}) = 1 \cdot P(A) + 0 \cdot (1 - P(A)) = P(A)$$

Это означает, что **оценивание вероятности события A эквивалентно оцениванию математического ожидания индикатора** $I\{A\}$.

### Дисперсия

Дисперсия индикаторной переменной определяется формулой:

$$\text{Var}(Y) = \text{Var}(I\{A\}) = E[I\{A\}^2] - E^2[I\{A\}] = P(A) - P^2(A) = P(A)(1 - P(A))$$

## Общая форма моделей бинарного выбора

Все модели бинарного выбора имеют общую структуру:

$$P(Y_i = 1 | x) = F(x_i'\beta)$$

где $F(\cdot)$ - некоторая функция распределения, принимающая значения от 0 до 1.

Основные типы моделей различаются выбором функции $F$:

- **Линейная модель вероятности**: $F(x_i'\beta) = x_i'\beta$
- **Probit-модель**: $F(x_i'\beta) = \Phi(x_i'\beta)$ (стандартное нормальное распределение)
- **Logit-модель**: $F(x_i'\beta) = \Lambda(x_i'\beta)$ (логистическое распределение)


## Линейная модель вероятности

### Спецификация модели

В линейной модели вероятности зависимая переменная моделируется как:

$$Y_i = x_i'\beta + \varepsilon_i, \quad i = 1, \ldots, n$$

Поскольку зависимая переменная принимает только значения 0 или 1, и $E(\varepsilon_i) = 0$, получаем:

$$E(Y_i) = 1 \cdot P(Y_i = 1) + 0 \cdot P(Y_i = 0) = P(Y_i = 1) = x_i'\beta$$

### Недостатки линейной модели вероятности

Несмотря на простоту применения метода наименьших квадратов, линейная модель вероятности имеет существенные недостатки:

#### 1. Ненормальность ошибок

Случайная ошибка $\varepsilon_i$ не распределена нормально, поскольку может принимать только два значения:

- $\varepsilon_i = 1 - x_i'\beta$ с вероятностью $P(Y_i = 1)$
- $\varepsilon_i = -x_i'\beta$ с вероятностью $P(Y_i = 0)$


#### 2. Гетероскедастичность

Дисперсия ошибки зависит от значений регрессоров:

$$\text{Var}(\varepsilon_i) = x_i'\beta(1 - x_i'\beta)$$

#### 3. Проблема интерпретации

Наиболее серьезная проблема заключается в том, что прогнозные значения $\hat{y}_i = x_i'\hat{\beta}$ могут выходить за пределы интервала , что делает их неинтерпретируемыми как вероятности.

### Практический пример

Рассмотрим модель вероятности сдачи зачета студентом:

- $Y_i$ = 1, если i-й студент сдал зачет, 0 в противном случае
- $x_i$ = время подготовки i-го студента (в часах)

Результаты оценивания показывают:

$$\hat{p} = -0.3 + 0.1x_i$$

Интерпретация: увеличение времени подготовки на один час увеличивает вероятность сдачи зачета на 0.1 (или 10 процентных пунктов).

## Logit и Probit модели

### Общая спецификация

Для устранения недостатков линейной модели вероятности используются нелинейные модели:

$$P(Y_i = 1) = F(x_i'\beta), \quad \forall x \in \mathbb{R}$$

### Probit-модель

В probit-модели используется функция распределения стандартного нормального распределения:

$$F(x_i'\beta) = \Phi(x_i'\beta) = \int_{-\infty}^{x_i'\beta} \frac{1}{\sqrt{2\pi}} e^{-t^2/2} dt$$

### Logit-модель

В logit-модели используется логистическая функция распределения:

$$F(x_i'\beta) = \Lambda(x_i'\beta) = \frac{e^{x_i'\beta}}{1 + e^{x_i'\beta}}$$

## Концепция латентной переменной

### Основная идея

Модели бинарного выбора можно интерпретировать через концепцию латентной (ненаблюдаемой) переменной $y_i^*$:

**Латентная переменная**:
$$y_i^* = x_i'\beta + \epsilon_i$$

**Наблюдаемая переменная**:

$$
Y_i = \begin{cases}
1, & \text{если } y_i^* > 0 \\
0, & \text{если } y_i^* \leq 0
\end{cases}
$$

### Вероятность события

Вероятность наступления события выражается через распределение ошибки:

$$P(Y_i = 1) = P(y_i^* > 0) = P(\epsilon_i > -x_i'\beta) = 1 - F_\epsilon(-x_i'\beta) = F_\epsilon(x_i'\beta)$$

где последнее равенство справедливо для симметричных распределений.

### Экономическая интерпретация: разность полезностей

В экономической теории латентная переменная часто интерпретируется как разность полезностей между альтернативами:

**Полезности альтернатив**:

- $U_i(1) = x_{1i}'\beta_1 + \epsilon_{1i}$ (полезность от выбора альтернативы 1)
- $U_i(0) = x_{2i}'\beta_2 + \epsilon_{2i}$ (полезность от выбора альтернативы 0)

**Правило выбора**:

$$
Y_i = \begin{cases}
1, & \text{если } U_i(1) > U_i(0) \\
0, & \text{если } U_i(1) \leq U_i(0)
\end{cases}
$$

**Латентная переменная как разность полезностей**:
$$y_i^* = U_i(1) - U_i(0) = x_i'\beta + \epsilon_i$$

## Оценивание методом максимального правдоподобия

### Функция правдоподобия

Поскольку каждое наблюдение $Y_i$ имеет распределение Бернулли, функция правдоподобия для выборки записывается как:

$$L(\beta) = \prod_{i:Y_i=1} P(Y_i = 1) \prod_{i:Y_i=0} P(Y_i = 0) = \prod_{i=1}^n F_i^{y_i}(1-F_i)^{1-y_i}$$

где $F_i = F(x_i'\beta)$.

### Логарифмическая функция правдоподобия

$$\ln L(\beta) = \sum_{i=1}^n [y_i \ln F_i + (1-y_i) \ln(1-F_i)]$$

### Условия первого порядка

Условие максимизации функции правдоподобия:

$$\frac{\partial \ln L}{\partial \beta} = \sum_{i=1}^n \frac{y_i - F_i}{F_i(1-F_i)} f_i x_i = \sum_{i=1}^n \tilde{\varepsilon}_i x_i = 0$$

где $\tilde{\varepsilon}_i = \frac{y_i - F_i}{F_i(1-F_i)} f_i$ - обобщенные остатки.

### Информационная матрица

$$I(\beta) = -E\left[\frac{\partial \ln L}{\partial \beta} \frac{\partial \ln L}{\partial \beta'}\right] = -E\left[\frac{\partial^2 \ln L}{\partial \beta \partial \beta'}\right]$$

## Свойства ML-оценок

ML-оценки параметров моделей бинарного выбора обладают следующими важными свойствами:

### 1. Состоятельность

Добавление информации приближает оценки к истинным значениям параметров.

### 2. Асимптотическая несмещенность

В среднем значение оценки совпадает с истинным параметром, что важно для проверки гипотез.

### 3. Асимптотическая эффективность

Обеспечивает оценку ковариационной матрицы параметров.

### 4. Асимптотическая нормальность

Позволяет проводить статистические тесты.

### 5. Инвариантность

Дает возможность получать оценки для функций от параметров: вероятностей, предельных эффектов и т.д.

## Сравнение логистического и нормального распределений

| Характеристика | Logit-модель | Probit-модель |
| :-- | :-- | :-- |
| Распределение | Логистическое Λ(0,1) | Стандартное нормальное N(0,1) |
| Функция распределения | $F(t) = \frac{e^t}{1 + e^t}$ | $F(t) = \int_{-\infty}^t \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx$ |
| Функция плотности | $f(t) = \frac{e^t}{(1 + e^t)^2}$ | $f(t) = \frac{1}{\sqrt{2\pi}} e^{-t^2/2}$ |
| Дисперсия | $\frac{\pi^2}{3} \approx 3.29$ | 1 |

## Специальные свойства логистической модели

### Шансы (odds)

Важным свойством logit-модели является простая интерпретация через шансы:

$$\ln\left(\frac{P(Y_i = 1)}{P(Y_i = 0)}\right) = x_i'\beta$$

### Отношение шансов (odds ratio)

Для бинарной объясняющей переменной X отношение шансов при фиксированных значениях других переменных $Z_1, \ldots, Z_p$ определяется как:

$$\exp(\beta_x) = \frac{P(Y=1|X=1,Z_1,\ldots,Z_p)/P(Y=0|X=1,Z_1,\ldots,Z_p)}{P(Y=1|X=0,Z_1,\ldots,Z_p)/P(Y=0|X=0,Z_1,\ldots,Z_p)}$$

### Свойство средней предсказанной вероятности

Для logit-модели выполняется важное свойство:

$$\frac{1}{n}\sum_{i=1}^n \hat{P}(Y_i = 1) = \frac{1}{n}\sum_{i=1}^n Y_i$$

Средняя предсказанная вероятность равна доле единиц в выборке.

## Интерпретация результатов: предельные эффекты

### Предельный эффект непрерывной переменной

Степень влияния непрерывной объясняющей переменной на вероятность события определяется предельным эффектом:

$\frac{\partial P(Y = 1)}{\partial x} = f(x'\beta)\beta$

### Предельный эффект бинарной переменной

Для бинарной переменной d предельный эффект рассчитывается как:

$$\Delta P(Y = 1|x) \approx P(Y = 1|d = 1, x) - P(Y = 1|d = 0, x) = F(x'\beta + \beta_d) - F(x'\beta)$$

### Различия в расчете предельных эффектов

**Средний предельный эффект (AME)**:
$$AME_k = \frac{1}{n}\sum_{i=1}^n \frac{\partial P(Y = 1|x)}{\partial x_k} = \frac{1}{n}\sum_{i=1}^n f(x_i'\beta)\beta_k$$

**Предельный эффект в средней точке (MEM)**:
$$MEM_k = \frac{\partial P(Y = 1|x = \bar{x})}{\partial x_k} = f(\bar{x}'\beta)\beta_k$$

**Приближенная формула для AME в logit-модели**:
$$AME_k \approx \frac{\sum_{i=1}^n Y_i}{n}\left(1 - \frac{\sum_{i=1}^n Y_i}{n}\right)\beta_k$$

## Практический пример: логит-модель

### Спецификация модели

Рассмотрим оценку вероятности сдачи зачета с помощью logit-модели:

$$\hat{P}(y_i = 1) = \frac{1}{1 + e^{-(-9 + 0.5x_i)}}$$

### Расчет предельных эффектов

Предельный эффект времени подготовки:

$$\frac{dP(y_i = 1)}{dx} = \frac{e^{-\beta_1 + \beta_2 x}}{(1 + e^{-\beta_1 + \beta_2 x})^2} \cdot \beta_2$$

**Для студента, готовящегося 15 часов**:
$$\frac{d\hat{P}(y_i = 1)}{dx} = \frac{e^{-(-9 + 0.5 \cdot 15)}}{(1 + e^{-(-9 + 0.5 \cdot 15)})^2} \cdot 0.5 = 0.07$$

Дополнительный час подготовки увеличивает вероятность сдачи зачета на 7 процентных пунктов.

**Для студента, готовящегося 100 часов**:
$$\frac{d\hat{P}(y_i = 1)}{dx} \approx 8 \times 10^{-19}$$

Для хорошо подготовленного студента дополнительный час подготовки практически не влияет на вероятность успеха.

## Прикладной пример: кредитоспособность российских банков

### Постановка задачи

Исследование факторов некредитоспособности российских банков на основе данных за 1999 год (182 наблюдения):

**Зависимая переменная**: 1 - банк с низким кредитным рейтингом (критическое состояние), 0 - иначе.

### Объясняющие переменные

- **TOTLIAB** - суммарные обязательства (тыс. руб.)
- **CURRENCY** - валютная составляющая (%)
- **EQUITY/ASS** - отношение недвижимости к чистым активам
- **PROFIT/ASS** - отношение прибыли к активам
- **RETAIL/TOTLIAB** - доля средств частных лиц в обязательствах
- **TOTLIAB/PREF** - отношение обязательств к рисковым активам


### Результаты оценивания

| Переменная | Коэффициент | Станд. ошибка | t-статистика | P-значение |
| :-- | :-- | :-- | :-- | :-- |
| Константа | -3.19 | 0.87 | -3.68 | 0.000 |
| TOTLIAB | 2.14e-07 | 7.21e-08 | 2.97 | 0.003 |
| CURRENCY | -0.096 | 0.031 | -3.13 | 0.021 |
| EQUITY/ASS | -8.58 | 3.71 | -2.31 | 0.022 |
| PROFIT/ASS | -26.99 | 9.49 | -2.84 | 0.005 |
| RETAIL/TOTLIAB | 3.53 | 1.23 | 2.87 | 0.005 |
| TOTLIAB/PREF | 2.00 | 0.90 | 2.23 | 0.027 |

## Тестирование гипотез

### Общая постановка

Для проверки линейных ограничений вида:

- $H_0: Q\beta = r$ (основная гипотеза)
- $H_1: Q\beta \neq r$ (альтернативная гипотеза)

где $Q$ и $r$ - матрица и вектор ограничений соответствующих размерностей.

### Три основных теста

Все тесты асимптотически эквивалентны и при выполнении $H_0$ имеют распределение $\chi^2_m$:

**Тест Вальда**:
$$(Q\hat{\beta} - r)^T [Q I^{-1}(\hat{\beta}) Q^T]^{-1} (Q\hat{\beta} - r) \sim_{H_0} \chi^2_m$$

**Тест отношения правдоподобия (LR)**:
$$2[\ln(L) - \ln(L_{H_0})] \sim_{H_0} \chi^2_m$$

**Тест множителей Лагранжа (LM)**:
$$s^T I^{-1}(\hat{\beta}_{H_0}) s \sim_{H_0} \chi^2_m$$

где $s$ - вектор производных логарифмической функции правдоподобия.

## Прогнозирование

### Прогнозирование вероятности

$$\hat{P}(Y_0 = 1) = F(x_0'\hat{\beta})$$

### Прогнозирование события

$$
Y_0 = \begin{cases}
1, & \text{если } F(x_0'\hat{\beta}) > c \\
0, & \text{если } F(x_0'\hat{\beta}) \leq c
\end{cases}
$$

где $c$ - пороговое значение. При равноценных ошибках классификации $c = 0.5$.

**Наивная модель**: всегда предсказывать наиболее часто встречающийся исход.

## Критерии качества модели

### ROC-кривая и AUC

**ROC-кривая** отображает соотношение между:

- **TPR** (True Positive Rate) - доля правильно классифицированных положительных случаев
- **FPR** (False Positive Rate) - доля ошибочно классифицированных отрицательных случаев

**AUC** (Area Under ROC Curve) - площадь под ROC-кривой:

- AUC = 0.5 соответствует случайному угадыванию
- Чем выше AUC, тем качественнее классификатор


### Псевдо-R²

**McFadden R²**:
$$R^2 = 1 - \frac{\ln L_{full}}{\ln L_{intercept}}$$

**Cox and Snell R²**:
$$R^2 = 1 - \left(\frac{L_{intercept}}{L_{full}}\right)^{2/n}$$

### Выбор спецификации

Для выбора между моделями с различным набором факторов используются информационные критерии AIC и BIC. Предпочтение отдается модели с меньшим значением критерия.

## Заключение

Модели бинарного выбора представляют собой мощный инструмент эконометрического анализа для исследования решений типа "да/нет". Logit и probit модели успешно устраняют основные недостатки линейной модели вероятности, обеспечивая корректные оценки вероятностей в интервале . Метод максимального правдоподобия позволяет получить состоятельные, асимптотически эффективные и нормально распределенные оценки параметров. Правильная интерпретация результатов через предельные эффекты и отношения шансов обеспечивает содержательную экономическую интерпретацию полученных результатов.

# Лекция 06. Модели множественного выбора

## Введение в модели множественного выбора

Модели множественного выбора представляют собой расширение бинарных моделей на случай, когда зависимая переменная может принимать более двух дискретных значений. Эти модели широко применяются в эконометрическом анализе для исследования ситуаций, где индивиды или экономические агенты выбирают между несколькими альтернативами.

## Практические примеры применения

Модели множественного выбора находят широкое применение в различных областях экономических исследований:

- **Профессиональный выбор**: Анализ решений о выборе профессии между инженером, научным работником, преподавателем
- **Транспортная экономика**: Исследование выбора способа передвижения (пешком, на автомобиле, на метро)
- **Экономика труда**: Моделирование выбора между работой, учебой и экономической неактивностью
- **Потребительское поведение**: Анализ выбора между различными брендами товаров или услуг


## Концепция случайной полезности

### Теоретические основы

Модели множественного выбора основываются на теории случайной полезности (Random Utility Theory). Предполагается, что имеется $m$ альтернатив, и для индивидуума $t$ альтернатива $j$ имеет полезность:

$$U_{tj} = u_{tj} + \varepsilon_{tj}$$

где:

- $u_{tj}$ — неслучайная (детерминированная) составляющая полезности
- $\varepsilon_{tj}$ — случайная составляющая полезности


### Правило выбора

Индивидуум $t$ выберет альтернативу $j$, если она обеспечивает максимальную полезность:

$$U_{tj} > U_{tk} \text{ для любого } k \neq j$$

Вероятность выбора альтернативы $j$ определяется как:

$$P(y_t = j) = P(u_{tj} + \varepsilon_{tj} > u_{tk} + \varepsilon_{tk}) \text{ для всех } k \neq j, k = 1, \ldots, m$$

## Logit-модель множественного выбора (Multinomial Logit)

### Основная формула

При предположении о логистическом распределении случайных ошибок вероятность выбора альтернативы $j$ имеет вид:

$$P(y_t = j) = \frac{\exp(u_{tj})}{\exp(u_{t1}) + \cdots + \exp(u_{tm})}$$

Предполагая линейную зависимость полезности от наблюдаемых характеристик:

$$u_{tj} = x_{tj}'\beta$$

получаем основную формулу multinomial logit модели:

$$P(y_t = j) = \frac{\exp(x_{tj}'\beta)}{\exp(x_{t1}'\beta) + \cdots + \exp(x_{tm}'\beta)}$$

### Альтернативная спецификация

Часто рассматривается модель, когда экзогенные переменные не зависят от альтернативы, а коэффициенты могут различаться:

$$u_{tj} = x_t'\beta_j$$

В этом случае:

$$P(y_t = j) = \frac{\exp(x_t'\beta_j)}{\exp(x_t'\beta_1) + \cdots + \exp(x_t'\beta_m)}$$

### Проблема идентифицируемости

Поскольку правая часть зависит от разности коэффициентов $\beta_2 - \beta_1, \ldots, \beta_m - \beta_1$, вводится нормировка $\beta_1 = 0$. Тогда:

$$P(y_t = 1) = \frac{1}{1 + \exp(x_t'\beta_2) + \cdots + \exp(x_t'\beta_m)}$$

$$P(y_t = j) = \frac{\exp(x_t'\beta_j)}{1 + \exp(x_t'\beta_2) + \cdots + \exp(x_t'\beta_m)}, \quad j \geq 2$$

## Свойство независимости от посторонних альтернатив (IIA)

### Определение свойства

Важным ограничением multinomial logit модели является предположение о независимости от посторонних альтернатив. Это свойство означает, что отношение вероятностей выбора любых двух альтернатив не зависит от наличия или отсутствия других альтернатив:

$$\frac{P(y_t = j)}{P(y_t = k)} = \frac{\exp(u_{tj})}{\exp(u_{tk})}$$

для любых $j, k = 1, \ldots, m$.

### Проблемы с предположением IIA

Это предположение может быть нереалистичным, если альтернативы достаточно близкие. Классический пример: если первая альтернатива — поездка на личном автомобиле, а вторая — поездка на красном автобусе, то отношение $P(y=1)/P(y=2)$ не должно зависеть от того, какой будет третья альтернатива (синий автобус или метро).

## Тест Хаусмана на независимость от посторонних альтернатив

### Основная идея теста

Тест Хаусмана-МакФаддена (1984) основан на предположении, что если подмножество вариантов выбора действительно независимо, то исключение его из модели систематически не изменит оценки параметров.

### Статистика теста

$$\chi^2 = (\hat{\beta}_s - \hat{\beta}_f)'[V_s - V_f]^{-1}(\hat{\beta}_s - \hat{\beta}_f) \sim \chi^2_K$$

где:

- $\hat{\beta}_s$ — оценки на ограниченном подмножестве альтернатив
- $\hat{\beta}_f$ — оценки на полном множестве вариантов выбора
- $V_s, V_f$ — соответствующие оценки асимптотических ковариационных матриц
- $K$ — количество факторов в модели с учетом константы


## Интерпретация результатов

### Предельные эффекты

В отличие от бинарных моделей, интерпретация коэффициентов в multinomial logit требует расчета предельных эффектов для каждой альтернативы. Предельный эффект переменной $x_k$ на вероятность выбора альтернативы $j$ равен:

$$\frac{\partial P(y_t = j)}{\partial x_k} = P(y_t = j)[\beta_{jk} - \sum_{l=1}^m P(y_t = l)\beta_{lk}]$$

### Критерии качества модели

Для оценки качества модели используются следующие критерии:

- **Процент правильно предсказанных значений**: сравнение предсказанных и фактических выборов
- **Логарифмическая функция правдоподобия**: для сравнения моделей с разными регрессорами
- **Псевдо-R²**: аналог коэффициента детерминации для категориальных моделей


## Практический пример

### Анализ трудового статуса

Рассмотрим пример из работы Wooldridge (2020), где анализируется выбор между тремя альтернативами трудового статуса:

- Status = 0: учащийся
- Status = 1: не работает и не учится
- Status = 2: работающий

Из 1,717 наблюдений: 99 учатся в школе, 332 находятся дома, 1,286 работают.

### Результаты интерпретации

Общий процент правильно предсказанных значений составил около 80%. Модель лучше всего предсказывает работающих мужчин (95,2%), чем учащихся (12,1%) или неработающих (39,2%).

**Пример интерпретации**: Два чернокожих мужчины с 5 годами стажа — мужчина с 16 годами образования имеет вероятность быть работающим на 0,042 выше, чем мужчина с 12 годами образования.

## Порядковые (Ordered) модели

### Концепция порядковых моделей

Порядковые модели применяются, когда альтернативы можно проранжировать по некоторому критерию. Примеры применения:

- **Туристический выбор**: отдых на даче < отдых в Сочи < отдых в Турции
- **Кредитные рейтинги банков**: от низшего к высшему
- **Удовлетворенность работой**: не удовлетворен < скорее не удовлетворен < скорее удовлетворен < удовлетворен


### Математическая формулировка

Выбор места отдыха, описываемый переменной $y$, зависит от латентной переменной $y^*$ (например, текущих накоплений):

$$
y = \begin{cases}
1, & \text{если } y^* \leq c_1 \\
2, & \text{если } c_1 < y^* \leq c_2 \\
3, & \text{если } y^* > c_2
\end{cases}
$$

где $c_1, c_2$ — пороговые уровни.

### Определение вероятностей

Предполагая $y_t^* = x_t'\beta + \varepsilon_t$ с нормированной дисперсией $\sigma^2 = 1$, получаем:

$$P(y_t = 1) = F(c_1 - x_t'\beta)$$

$$P(y_t = 2) = F(c_2 - x_t'\beta) - F(c_1 - x_t'\beta)$$

$$P(y_t = 3) = 1 - F(c_2 - x_t'\beta)$$

Выбор функции $F(\cdot)$ определяет тип модели: нормальное распределение дает ordered probit, логистическое — ordered logit.

### Общий случай с $m$ альтернативами

Для $m$ альтернатив с границами $-\infty = c_0 < c_1 < \cdots < c_{m-1} < c_m = \infty$ латентная переменная определяется как:

$$y_t^* = x_t'\beta + \varepsilon_t$$

Вероятности имеют вид:

$$P(y_t = j) = P(c_{j-1} < y_t^* \leq c_j) = F(c_j - x_t'\beta) - F(c_{j-1} - x_t'\beta), \quad j = 1, \ldots, m$$

### Функция правдоподобия

$$L = \prod_{j=1}^m \prod_{t:y_t=j} [F(c_j - x_t'\beta) - F(c_{j-1} - x_t'\beta)]$$

## Предельные эффекты в порядковых моделях

Предельные эффекты рассчитываются отдельно для каждой категории:

### Для первой категории

$$\frac{\partial P(y_t = 1|x)}{\partial x_{t,k}} = -\beta_k \phi(c_1 - x_t'\beta)$$

### Для промежуточных категорий

$$\frac{\partial P(y_t = j|x)}{\partial x_{t,k}} = \beta_k[\phi(c_{j-1} - x_t'\beta) - \phi(c_j - x_t'\beta)], \quad 1 < j < m$$

### Для последней категории

$$\frac{\partial P(y_t = m|x)}{\partial x_{t,k}} = \beta_k \phi(c_{m-1} - x_t'\beta)$$

## Особенности интерпретации порядковых моделей

### Ключевые моменты интерпретации

1. **Необходимость представления границ**: Обязательно нужно представлять оценки пороговых значений $c_s$ для корректного предсказания категорий
2. **Ограниченная информативность коэффициентов**: Коэффициенты модели сами по себе мало информативны без расчета предельных эффектов
3. **Совпадение знаков**: Только для последней категории знаки предельных эффектов и коэффициентов совпадают
4. **Противоположные знаки**: Для первой категории знаки коэффициентов и предельных эффектов противоположны
5. **Неопределенность для промежуточных категорий**: Для промежуточных категорий знак предельного эффекта зависит от разности:
$$\phi(c_{j-1} - x_t'\beta) - \phi(c_j - x_t'\beta)$$

## Предположение о параллельности

### Определение предположения

Ключевое предположение порядковых моделей состоит в том, что все коэффициенты $\beta$ одинаковы для различных категорий. Это означает, что:

$$P(Y_t \leq k|X) = F(c_k - x_i'\beta), \quad k = 1, \ldots, M$$

### Тест Бранта

Для проверки предположения о параллельности используется тест Бранта (1990). Конструируются бинарные переменные:

$$
w_{ij} = \begin{cases}
1, & \text{если } y_{ij} \leq j \\
0, & \text{в противном случае}
\end{cases}, \quad j = 1, 2, \ldots, J-1
$$

**Нулевая гипотеза**: $H_0: \beta_1 = \beta_2 = \cdots = \beta_{J-1}$

**Статистика Вальда**:
$$W = (R\hat{\beta}^*)'[R \times \text{Asy.Var}(\hat{\beta}^*) \times R']^{-1}(R\hat{\beta}^*) \sim \chi^2_{(J-2)K}$$

где $\hat{\beta}^*$ — оценки индивидуальных probit/logit моделей без свободного члена.

### Проблемы при нарушении предположения

Если нулевая гипотеза теста Бранта отвергается, возможные причины:

1. **Неверная спецификация** латентной переменной $x'\beta$
2. **Гетероскедастичность** в случайной ошибке $\varepsilon$
3. **Неверный выбор** функции распределения (logit/probit)

## Практический пример: анализ рейтингов российских банков

### Описание исследования

Исследование Пересецкого, Карминского, ван Суста (2003) по анализу кредитных рейтингов российских банков. Зависимая переменная — кредитный рейтинг банка (1 — самый высокий).

### Объясняющие переменные и результаты

| Переменная | Коэффициент | Описание |
| :-- | :-- | :-- |
| BP/SK | -0.132 | Прибыльность капитала |
| DOSTKAP | 0.054 | Достаточность капитала (Норматив Н1 ЦБ РФ) |
| DKE/VB | 2.904 | Доля долгосрочных кредитов экономике в валюте банка |
| MGLIK | -0.019 | Мгновенная ликвидность (Норматив Н2 ЦБ РФ) |
| SK | $-7.63 \times 10^{-7}$ | Собственный капитал |

### Интерпретация результатов

Все коэффициенты, за исключением DKE/VB, значимы на 1%-ном уровне, коэффициент при DKE/VB значим на 5%-ном уровне. Знаки коэффициентов согласуются с экономической интуицией: поскольку уменьшение зависимой переменной соответствует повышению рейтинга, отрицательный знак коэффициента означает, что увеличение соответствующего фактора способствует повышению надежности банка.

## Заключение

Модели множественного выбора представляют собой мощный инструментарий для анализа дискретных решений экономических агентов. Multinomial logit модели подходят для анализа несвязанных альтернатив, тогда как порядковые модели эффективны при наличии естественного ранжирования вариантов. Ключевыми аспектами успешного применения являются тестирование предположений модели (особенно IIA для multinomial logit и параллельности для ordered моделей), корректный расчет предельных эффектов и содержательная экономическая интерпретация результатов.

# Лекция 07. Модели с ограниченными значениями

## Введение

Модели с ограниченными значениями зависимой переменной представляют собой важный класс эконометрических моделей, используемых в ситуациях, когда наблюдаемые данные не полностью отражают весь диапазон возможных значений исследуемой переменной. Эти модели критически важны для корректного анализа экономических явлений, где данные могут быть усечены или цензурированы.


## Усеченные (truncated) выборки

### Определение и концепция

**Усечение** — это характеристика распределения, с помощью которой получаются выборки. При усечении мы наблюдаем только те значения случайной величины, которые удовлетворяют определенному условию. Например, если имеются данные о доходах людей только выше черты бедности, нельзя делать выводы относительно всей генеральной совокупности.

### Концепция латентной переменной

Усеченные выборки моделируются через концепцию латентной (ненаблюдаемой) переменной $y_i^*$:

$$
y_i = \begin{cases}
y_i^*, & \text{если } y_i^* \geq a \\
\text{не наблюдаем}, & \text{если } y_i^* < a
\end{cases}
$$

где $a$ — пороговое значение усечения.

### Условная плотность распределения

Плотность наблюдаемой переменной равна условной плотности латентной переменной:

$$
f(y) = \begin{cases}
\frac{f_{y^*}(y)}{P(y_i^* \geq a)}, & \text{если } y \geq a \\
0, & \text{если } y < a
\end{cases}
$$

### Доказательство формулы условной плотности

Для доказательства используем определение производной функции распределения:

$$f(t) = F_y'(t) = \left(P(y \leq t)\right)' = \left(P(y^* \leq t | y^* \geq a)\right)'$$

$$= \left(\frac{P(a \leq y^* \leq t)}{P(y^* \geq a)}\right)' = \frac{(F_{y^*}(t) - F(a))'}{P(y^* \geq a)} = \frac{f_{y^*}(t)}{P(y^* \geq a)}$$

если $t \geq a$, и $f(t) = 0$ если $t < a$.

## Усеченное стандартное нормальное распределение

### Практический пример

Рассмотрим случай, когда $y^* \sim N(0,1)$, $a = -1$:

$$
y = \begin{cases}
y^*, & \text{если } y^* \geq -1 \\
\text{не наблюдаем}, & \text{если } y^* < -1
\end{cases}
$$

Плотность усеченного распределения:

$$
f(y) = \begin{cases}
\frac{1}{0.84} \varphi(y), & \text{если } y \geq -1 \\
0, & \text{если } y < -1
\end{cases}
$$

поскольку $P(y^* \geq a) = 1 - \Phi(-1) = \Phi(1) = 0.84$.

## Моменты усеченного нормального распределения

### Формулы для математического ожидания и дисперсии

Пусть $Y \sim N(\mu, \sigma^2)$, тогда при усечении:

$$E(y | \text{условие}) = \mu + \sigma \lambda(\alpha)$$

$$\text{Var}(y | \text{условие}) = \sigma^2(1 - \delta(\alpha))$$

где:

- $\lambda(\alpha) = \frac{\varphi(\alpha)}{1 - \Phi(\alpha)} > 0$ для условия $y_i^* \geq a$ (положительное смещение)
- $\lambda(\alpha) = -\frac{\varphi(\alpha)}{\Phi(\alpha)} < 0$ для условия $y_i^* < a$ (отрицательное смещение)
- $\delta(\alpha) = \lambda(\alpha)(\lambda(\alpha) - \alpha)$
- $\alpha = \frac{a - \mu}{\sigma}$


### Ключевые выводы

1. **При усечении снизу** среднее для усеченного распределения больше, чем для исходного
2. **При усечении сверху** среднее для усеченного распределения меньше, чем для исходного
3. **Усечение всегда уменьшает дисперсию** по сравнению с дисперсией исходного распределения

## Практический пример: доход богатых американцев

### Постановка задачи

По данным New York Post (1987), доход «типичного богатого американца» составлял \$142,000 по обследованию домохозяйств с доходом выше \$100,000. Требуется определить доход «типичного американца» при предположении о логнормальном распределении доходов.

### Решение

Составляется система уравнений:

$$E(\ln(y) | \ln(y) > \ln(100)) = \ln(142) = \mu + \sigma \frac{\varphi\left(\frac{\ln(100) - \mu}{\sigma}\right)}{1 - \Phi\left(\frac{\ln(100) - \mu}{\sigma}\right)}$$

$$P(\ln(y) > \ln(100)) = 1 - \Phi\left(\frac{\ln(100) - \mu}{\sigma}\right) = 0.02$$

Решение дает $\mu = 2.635$, $\sigma = 0.959$, откуда безусловное математическое ожидание дохода:

$$E(e^{\ln Y}) = e^{\mu + \sigma^2/2} = 22,087$$

что близко к официальной статистике США (\$25,000).

## Усеченная регрессионная модель

### Спецификация модели

Предпосылки классической линейной регрессионной модели выполняются для латентной переменной:

$$y_i^* = x_i'\beta + \varepsilon_i, \quad \varepsilon_i \sim N(0, \sigma^2)$$

$$E(y_i^*) = x_i'\beta$$

Однако для наблюдаемой переменной:

$$E(y_i | y_i^* \geq a) = x_i'\beta + E(\varepsilon_i | y_i^* \geq a) = x_i'\beta + \sigma \frac{\varphi\left(\frac{a - x_i'\beta}{\sigma}\right)}{1 - \Phi\left(\frac{a - x_i'\beta}{\sigma}\right)}$$

### Проблемы с МНК

При наличии усечения МНК неприменим по следующим причинам:

1. **Изменение распределения ошибки**: $E(\varepsilon | \text{условие усечения}) \neq 0$
2. **Нелинейная зависимость** от коэффициентов
3. **Потеря состоятельности** оценок

Поэтому необходимо использовать **метод максимального правдоподобия**.

## Предельные эффекты для усеченной модели

### Безусловное математическое ожидание

$$\frac{\partial E(y_i^*)}{\partial x} = \beta$$

Это привычная интерпретация коэффициентов, одинаковая для всех наблюдений.

### Условное математическое ожидание

$$\frac{\partial E(y_i | y_i^* > a)}{\partial x} = \beta(1 - \delta_i)$$

где $\delta_i = \lambda_i(\lambda_i - \alpha_i)$, $\lambda_i = \frac{\varphi(\alpha_i)}{1 - \Phi(\alpha_i)}$, $\alpha_i = \frac{a - x_i'\beta}{\sigma}$

Предельный эффект **всегда меньше коэффициента** по абсолютному значению и **зависит от наблюдения**.

## Прогнозы для усеченной модели

### Типы прогнозов

1. **Безусловное математическое ожидание**:
$$E(Y_0) = x_0'\beta$$
2. **Условное математическое ожидание**:
$$E(Y_0 | Y_0 > a) = x_0'\beta + \sigma\lambda_0$$
3. **Вероятность «попадания в выборку»**:
$$P(Y_0 > a) = 1 - \Phi\left(\frac{a - x_0'\beta}{\sigma}\right)$$

## Цензурированные данные

### Определение и примеры

**Цензурирование** отличается от усечения тем, что сохраняется частичная информация о наблюдениях, не удовлетворяющих условию. Классический пример — модель Тобина (1958) для расходов семей на автомобили:

$$
y_i = \begin{cases}
y_i^*, & \text{если } y_i^* > 0 \\
0, & \text{если } y_i^* \leq 0
\end{cases}
$$

где $y^*$ — «желаемые» расходы, $y_i^* = x_i'\beta + \varepsilon_i$, $\varepsilon_i \sim N(0, \sigma^2)$.

### Различия между усеченными и цензурированными выборками

| Характеристика | Усеченные выборки | Цензурированные выборки |
| :-- | :-- | :-- |
| Информация об исключенных наблюдениях | Отсутствует полностью | Частичная (значения X и цензурированные значения Y) |
| Наблюдаемые данные | Только удовлетворяющие условию | Все наблюдения с модификацией неудовлетворяющих |
| Тип распределения | Условное непрерывное | Смешанное дискретно-непрерывное |

## Тобит-модель

### Функция правдоподобия

Для цензурированных данных функция правдоподобия имеет вид:

$$\ln L = \sum_{y_i > 0} \left[-\frac{1}{2}\ln(2\pi) + \ln(\sigma^2) + \frac{(y_i - x_i'\beta)^2}{\sigma^2}\right] + \sum_{y_i = 0} \ln\left(1 - \Phi\left(\frac{x_i'\beta}{\sigma}\right)\right)$$

### Предельные эффекты в тобит-модели

Тобит-модель позволяет оценить три типа математических ожиданий:

1. **Латентной переменной**: $E(y_i^*) = x_i'\beta$
2. **Условного для положительных значений**: $E(y_i | y_i^* > 0) = x_i'\beta + \sigma\lambda_i$
3. **Цензурированной переменной**: $E(y_i) = \Phi_i \cdot (x_i'\beta + \sigma\lambda_i)$

Соответствующие предельные эффекты:


| Тип ожидания | Предельный эффект |
| :-- | :-- |
| Латентная переменная | $\beta$ |
| Условное (положительные) | $\beta(1 - \delta_i)$ |
| Цензурированная | $\beta\Phi_i$ |

### Декомпозиция Макдональда-Моффитта

Для стандартного нормального распределения с цензурированием в нуле предельный эффект можно разложить:

$$\frac{\partial E(y_i | x_i)}{\partial x_i} = P(\text{y}_i > 0) \frac{\partial E(y_i | x_i, y_i > 0)}{\partial x_i} + E(y_i | x_i, y_i > 0) \frac{\partial P(\text{y}_i > 0)}{\partial x_i}$$

Это показывает два эффекта изменения $x_i$: влияние на условное среднее в положительной части распределения и влияние на вероятность попадания в эту часть.

## Ограничения тобит-модели

### Основное ограничение

**Критическое предположение**: вероятность участия и интенсивность участия генерируются **одним и тем же процессом**:

$$y_i^* = x_i'\beta + \varepsilon_i$$

Это означает, что предельные эффекты переменных **совпадают по знаку** для вероятности участия ($\beta\varphi_i$) и условного математического ожидания ($\beta(1 - \delta_i)$).

### Пример нарушения условия

**Отдых с детьми на море**: количество детей может по-разному влиять на:

1. Семейные расходы во время поездки с детьми (может увеличивать)
2. Вероятность поездки с детьми (может уменьшать)

В таких случаях модель Тобина **несостоятельна**.

## Модель Хекмана (Тобин II)

### Основная идея

Модель Хекмана предполагает, что **вероятность участия и интенсивность участия генерируются разными процессами**:

**Уравнение участия**:
$$z_i^* = w_i'\gamma + u_i$$

$$
z_i = \begin{cases}
1, & \text{если } z_i^* \geq 0 \\
0, & \text{если } z_i^* < 0
\end{cases}
$$

**Уравнение интенсивности**:
$$y_i^* = x_i'\beta + \varepsilon_i$$

$$
y_i = \begin{cases}
y_i^*, & \text{если } z_i = 1 \\
\text{не наблюдаем}, & \text{если } z_i = 0
\end{cases}
$$

### Предпосылки модели

Случайные ошибки совместно нормально распределены:

$$\begin{pmatrix} \varepsilon_i \\ u_i \end{pmatrix} \sim N\left(\begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} \sigma^2 & \rho\sigma \\ \rho\sigma & 1 \end{pmatrix}\right)$$

### Несостоятельность МНК при корреляции ошибок

При $\rho \neq 0$ возникает смещение отбора:

$$E(y_i) = E(y_i^* | z_i = 1) = x_i'\beta + E(\varepsilon_i | u_i \geq -w_i'\gamma) = x_i'\beta + \rho\sigma\frac{\varphi(w_i'\gamma)}{\Phi(w_i'\gamma)}$$

Условное математическое ожидание ошибки **не равно нулю**, что приводит к несостоятельности оценок МНК.

## Оценивание модели Хекмана

### Двухшаговая процедура

**Шаг 1**: Оценивание уравнения участия (probit-модель):
$$P(z_i = 1) = P(w_i'\gamma + u_i > 0) = \Phi(w_i'\gamma)$$

Вычисление обратного отношения Миллса (лямбда Хекмана):
$$\hat{\lambda}_i = \frac{\varphi(w_i'\hat{\gamma})}{\Phi(w_i'\hat{\gamma})}$$

**Шаг 2**: Оценивание уравнения интенсивности с поправкой на смещение:
$$E(y_i^* | z_i = 1) = x_i'\beta + \rho\sigma\hat{\lambda}_i$$

### Метод максимального правдоподобия

Альтернативно можно использовать ММП для совместного оценивания обеих уравнений, что обеспечивает большую эффективность оценок.

## Выбор между моделями

### Тесты спецификации

**МНК против модели Хекмана**:

- $H_0: \rho = 0$ (смещения отбора нет)
- $H_1: \rho \neq 0$ (есть смещение отбора)

**Модель Хекмана против Тобит**:

- $H_0: \rho = 1, \beta = \sigma\gamma$ (одинаковые процессы)
- $H_1: \rho \neq 1$ или $\beta \neq \sigma\gamma$ (разные процессы)


### Практическое применение

При значимости коэффициента при $\hat{\lambda}_i$ следует использовать модель Хекмана. При его незначимости можно применять обычный МНК для оценки уравнения интенсивности.

## Практический пример: анализ предложения труда пенсионеров

### Описание исследования

На данных РМЭЗ-ВШЭ 2000 г. оценивается модель Хекмана для заработной платы работающих пенсионеров.

**Переменные**:

- Зависимая: wage (заработная плата), akt (1 - работает, 0 - не работает)
- Объясняющие: пол, возраст, образование, семейное положение, наличие детей и внуков, опыт работы


### Результаты

LR-тест гипотезы $\rho = 0$ дает p-value = 0.044, что указывает на значимость корреляции ошибок на 5%-ном уровне. Следовательно, уравнения зависимы и должны оцениваться совместно.

**Предельные эффекты**: каждый дополнительный год стажа увеличивает заработную плату работающих пенсионеров на 34.7 руб., а для всех пенсионеров (включая неработающих) — на 43.4 руб.

## Заключение

Модели с ограниченными значениями зависимой переменной представляют собой мощный инструментарий для анализа экономических данных в условиях неполной наблюдаемости. Правильный выбор между усеченными, цензурированными моделями и моделью Хекмана критически важен для получения несмещенных и состоятельных оценок параметров. Ключевым является понимание природы ограничений данных и экономических механизмов, лежащих в основе процессов участия и интенсивности.


# Лекция 08. Квантильная регрессия

## Введение в квантильную регрессию

Квантильная регрессия представляет собой мощный статистический метод, позволяющий оценивать условные квантили распределения зависимой переменной как функцию от независимых переменных. В отличие от метода наименьших квадратов (МНК), который фокусируется на условном среднем, квантильная регрессия позволяет исследовать влияние объясняющих переменных на различные части распределения зависимой переменной.

## Квантили в генеральной совокупности

### Определение квантиля

Для непрерывной случайной величины $y$ $q$-ой квантилью в генеральной совокупности называется число $\mu_q$, такое что значения $y$, меньшие или равные $\mu_q$, встречаются с вероятностью $q$:

$$q = P(y \leq \mu_q) = F_y(\mu_q)$$

где $F_y$ - функция распределения $y$. Следовательно:

$$\mu_q = F_y^{-1}(q)$$

### Популярные квантили

Наиболее часто используемыми квантилями являются:

- Медиана: $q = 0.5$
- Верхняя квартиль: $q = 0.75$
- Нижняя квартиль: $q = 0.25$

Для стандартного нормального распределения $\mu_{0.5} = 0$, $\mu_{0.95} = 1.645$, а $\mu_{0.975} = 1.960$. Термин "100$q$-тая процентиль" является синонимом $q$-той квантили.

### Квантили в регрессионных моделях

В регрессионной модели $q$-той квантилью в генеральной совокупности для $y$ при условии $x$ называют функцию $\mu_q(x)$, такую, что при условии $x$ вероятность того, что $y$ не превышает $\mu_q$, равна $q$:

$$\mu_q(x) = F_{y|x}^{-1}(q)$$

где $F_{y|x}$ - условная функция распределения $y$ при $x$.

## Медианная регрессия и метод наименьших абсолютных отклонений (LAD)

### МНК и условное среднее значение

В предпосылках классической линейной регрессионной модели (КЛРМ):

$$y = x'\beta + \varepsilon, \quad E(\varepsilon|x) = 0$$

$$E(y|x) = x'\beta$$

Если все величины $\varepsilon|x$ распределены нормально, то распределение $\varepsilon|x$ симметрично и медиана $M(\varepsilon|x) = 0$, а $M(y|x) = x'\beta$. Оценка МНК является естественным выбором для оценки условного среднего значения $y$.

### Метод наименьших абсолютных отклонений (LAD)

Задача МНК для оценки условного среднего $E(y|x) = x'\beta$:

$$\min_{\beta} \sum_{i=1}^{N} (y_i - x_i'\beta)^2$$

Задача метода наименьших абсолютных отклонений (Least Absolute Deviations, LAD) для оценки условной медианы $M(y|x) = x'\beta$:

$$\min_{\beta} \sum_{i=1}^{N} |y_i - x_i'\beta|$$

LAD представляет собой медианную квантильную регрессию.

### Сравнение МНК и LAD

Оценки коэффициентов $\beta$ методами МНК и LAD совпадают и являются состоятельными, если выполнены два условия:

1. Распределение $\varepsilon|x$ симметрично относительно нуля
2. $E(\varepsilon|x) = 0$

Если $\varepsilon|x$ имеет несимметричное распределение относительно нуля, то $M(y|x) = x'\beta + M(\varepsilon|x)$.

### Оценка стандартных ошибок регрессии LAD

Один из способов оценки стандартных ошибок регрессии LAD – метод бутстрэпа. Для LAD-оценки асимптотическая ковариационная матрица методом бутстрэпа выглядит так:

$$\text{asy. Var}(\hat{\beta}_{LAD}) = \frac{1}{R} \sum_{r=1}^{R} (\hat{\beta}_{LAD}^{(r)} - \hat{\beta}_{LAD})(\hat{\beta}_{LAD}^{(r)} - \hat{\beta}_{LAD})'$$

где $\hat{\beta}_{LAD}$ - оценки коэффициентов методом LAD, а $\hat{\beta}_{LAD}^{(r)}$ – $r$-я оценка LAD-величины $\beta$, основанная на $n$ наблюдениях, полученных с помощью выборки с возвращением из начальных данных (парный бутстрэп).

## Бутстрэп в задаче регрессии

### Виды бутстрэпа

В моделях регрессии применяются два основных вида бутстрэпа:

1. **Парный бутстрэп** (выборки с возвращением пар $(y;x)$)
2. **Параметрический бутстрэп**

### Парный бутстрэп

На примере модели $y_i = \beta_1 + \beta_x x_i + \beta_w w_i + u_i$:

1. Генерируем очередную бутстрэп-выборку $x_i^*, w_i^*, y_i^*$, где $i \in \{1, \ldots, n\}$, случайно выбирая $n$ наблюдений из исходной выборки с повторениями.
2. Считаем очередную бутстрэп-оценку коэффициента $\hat{\beta}_{xj}^*$ или $t$-статистику $t_j^* = \frac{\hat{\beta}_{xj}^* - \hat{\beta}_x}{se(\hat{\beta}_{xj}^*)}$.
3. Повторяем первые два шага много раз: $j = 1, \ldots, 10000$.

### Параметрический бутстрэп

1. Для бутстрэп-выборки сохраняем полностью исходную матрицу регрессоров $X$.
2. Генерируем бутстрэп-выборку для ошибок: $u_i^* \sim \mathcal{N}(0; \hat{\sigma}_u^2)$
3. Генерируем бутстрэп-выборку для зависимой переменной: $y_i^* = \hat{\beta}_1 + \hat{\beta}_x x_i + \hat{\beta}_w w_i + u_i^*$, используя $\hat{\beta}$ и $\hat{\sigma}_u^2$ исходной регрессии.
4. Считаем очередную бутстрэп-оценку коэффициента $\hat{\beta}_{xj}^*$ или $t$-статистику $t_j^* = \frac{\hat{\beta}_{xj}^* - \hat{\beta}_x}{se(\hat{\beta}_{xj}^*)}$.
5. Повторяем шаги два-четыре много раз: $j = 1, \ldots, 10000$.

## Преимущества и недостатки медианной регрессии

### Преимущества медианной регрессии над МНК

- Более богатые характеристики данных
- Более устойчива к выбросам, чем МНК
- Может быть применена для цензурированных данных
- Оценки LAD могут быть состоятельными при более слабых предположениях, чем МНК
- Преимущества LAD особенно значимы на малых выборках и в случае ненормального распределения случайных ошибок (более толстые хвосты)


### Недостатки медианной регрессии

- Нет явных формул для оценок коэффициентов и стандартных ошибок
- Только асимптотические свойства оценок коэффициентов


## Пример: оценка LAD производственной функции Кобба-Дугласа

### Спецификация модели

Zellner и Revankar (1970) рассматривали производственную функцию вида:

$$\ln \left( \frac{Y_i}{N_i} \right) = \beta_1 + \beta_2 \ln \left( \frac{K_i}{N_i} \right) + \beta_3 \ln \left( \frac{L_i}{N_i} \right) + \varepsilon_i$$

где:

- $Y$ – добавленная стоимость выпуска
- $K$ – капитал
- $L$ – труд
- $N$ – число сотрудников в транспортной отрасли


### Результаты оценивания

Анализ стандартизированных остатков выявил наличие выбросов (наблюдения для Флориды и Кентукки).


| Метод | Коэффициент | Оценка | Стандартная ошибка | t-статистика |
| :-- | :-- | :-- | :-- | :-- |
| МНК | Константа | 2,293 | 0,107 | 21,396 |
|  | $\beta_k$ | 0,279 | 0,081 | 3,458 |
|  | $\beta_l$ | 0,927 | 0,098 | 9,431 |
| LAD | Константа | 2,275 | 0,202 | 11,246 |
|  | $\beta_k$ | 0,261 | 0,124 | 2,099 |
|  | $\beta_l$ | 0,927 | 0,121 | 7,637 |
| LAD (бутстрэп) | Константа | 2,275 | 0,183 | 12,374 |
|  | $\beta_k$ | 0,261 | 0,138 | 1,881 |
|  | $\beta_l$ | 0,927 | 0,169 | 5,498 |

Оценки МНК без учета выбросов: (2,205; 0,261, 0,879) - значимо изменились! LAD не меняется при удалении этих наблюдений, что демонстрирует нечувствительность LAD к выбросам.

## Линейная модель квантильной регрессии

### Спецификация модели

Модель квантильной регрессии имеет вид:

$$Q_{y|x}(q) = x'\beta_q$$

такая, что:

$$\text{Prob}(y \leq x'\beta_q|x) = q, \quad 0 < q < 1$$

Оценка вектора коэффициентов $\beta_q$ для квантиля $q$ вычисляется путем минимизации функции:

$$Q_n(\beta_q) = \sum_{i:y_i \geq x_i'\beta_q}^{N} q|y_i - x_i'\beta_q| + \sum_{i:y_i < x_i'\beta_q}^{N} (1-q)|y_i - x_i'\beta_q|$$

Если $q = 0.5$, то получаем медианную регрессию (LAD).

### Нахождение оценок для квантильной регрессии

Целевая функция не является дифференцируемой, поэтому градиентные методы оптимизации не применимы. Вместо этого используются методы линейного программирования.

### Свойства оценок квантильных регрессий

Асимптотическое распределение оценок:

$$\sqrt{N}(\hat{\beta}_q - \beta_q) \xrightarrow{d} \mathcal{N}(0, A^{-1}BA^{-1})$$

где:

$$A = \text{plim} \frac{1}{N} \sum_{i=1}^{N} f_{u_q}(0|x_i) x_i'x_i, \quad B = \text{plim} \frac{1}{N} \sum_{i=1}^{N} q(1-q)x_i'x_i$$

а $f_{u_q}(0|x_i)$ - условная плотность распределения случайного члена $u_q = y - x'\beta_q$, подсчитанная в точке $u_q = 0$.

### Оценка стандартных ошибок для квантильной регрессии

Существуют два основных подхода:

1. **Асимптотические оценки**:
$$\text{Asy.Var}(b_q) = \frac{1}{n} H^{-1} G H^{-1}$$

где:
$$H = \text{plim} \frac{1}{n} \sum_{i=1}^{n} f_q(0|x_i) x_i x_i'$$
$$G = \text{plim} \frac{q(1-q)}{n} \sum_{i=1}^{n} x_i x_i'$$
2. **Методы бутстрэпа** (предпочтительнее, так как не требуют оценки плотности ошибок)

### Проверка гипотез

После получения оценок стандартных ошибок коэффициентов применяется t-статистика для проверки значимости коэффициентов:

$$t = \frac{\hat{\beta}_j - \beta_j}{se(\hat{\beta}_j)} \xrightarrow{d} N(0,1)$$

## Практические примеры применения квантильной регрессии

### Пример 1: Эластичность по кредитным картам в зависимости от дохода/расходов

Модель:

$$Q_{\ln Spending|x}(q) = \beta_{1,q} + \beta_{2,q} \ln Income + \beta_{3,q} Age + \beta_{4,q} Dependents$$

Результаты показывают, что эластичность расходов по доходам значительно различается по квантилям: от 1,40 для нижнего дециля до 0,84 для верхнего дециля. Это объясняется тем, что при высоком уровне расходов независимо от дохода существует (сравнительное) насыщение в реакции расходов на изменение дохода.

### Пример 2: Отдача на образование

Исследование показало, что отдача на образование значимо различается по квантилям распределения заработной платы. Особенно заметны различия для квантиля 0.9, а также между 1980 и 1990 годами.

### Пример 3: Оценка эластичности медицинских расходов по общим расходам

Исследование на данных Вьетнамского исследования качества жизни в 1997 году (N=5006) показало, что МНК-оценка эластичности составляет 0.57 (товар первой необходимости, спрос неэластичен по доходу). Однако квантильная регрессия выявила значительную гетерогенность этого показателя по разным уровням расходов.

## Неоднородность и гетероскедастичность

Коэнкер и Бассетт (1982) разработали квантильную регрессию как средство тестирования на гетероскедастичность, когда процесс, порождающий данные, является линейной моделью. Расхождение квантильных регрессий рассматривается как следствие гетероскедастичности.

## Заключение

Квантильная регрессия представляет собой мощный инструмент для анализа зависимостей между переменными, позволяющий исследовать влияние факторов на различные части распределения зависимой переменной. В отличие от МНК, который фокусируется только на условном среднем, квантильная регрессия дает более полную картину взаимосвязей, особенно в случаях асимметричных распределений, наличия выбросов и гетероскедастичности.

Основные преимущества квантильной регрессии включают устойчивость к выбросам, возможность анализа неоднородности эффектов по разным частям распределения и меньшую зависимость от предположений о распределении ошибок. Эти свойства делают квантильную регрессию незаменимым инструментом в современном эконометрическом анализе.


# Лекция 09. Модели счетных данных

## Введение в модели счетных данных

Модели счетных данных (count data models) представляют собой специализированный класс эконометрических моделей, предназначенных для анализа переменных, которые принимают неотрицательные целочисленные значения. Эти модели широко применяются в экономических исследованиях для анализа явлений, связанных с подсчетом количества событий за определенный период времени.


## Практические примеры счетных данных

Модели счетных данных находят широкое применение в различных областях экономических исследований:

- **Здравоохранение**: Число посещений врача индивидуумом за год
- **Финансы**: Количество негативных записей в кредитной истории заемщика
- **Инновации**: Количество выданных патентов компании или исследователю
- **Миграция**: Число мигрантов из страны A в страну B за определенный период
- **Демография**: Количество детей в семье

В каждом из этих примеров рассматриваемые переменные являются счетчиками числа определенных событий.

## Особенности счетных данных

### Основные характеристики

Счетчик представляет собой количественную меру, которая теоретически может анализироваться с помощью множественной регрессии. Однако счетные данные обладают рядом специфических особенностей, которые делают применение стандартных регрессионных методов неоптимальным:

1. **Преобладание нулевых и малых значений**: Во многих выборках значительная доля наблюдений равна нулю
2. **Дискретная природа**: Переменная может принимать только целочисленные неотрицательные значения
3. **Асимметричность распределения**: Данные часто скошены вправо
4. **Гетероскедастичность**: Дисперсия возрастает вместе со средним значением

### Проблемы с высокой долей нулевых значений

В некоторых исследованиях доля нулевых значений может достигать значительных величин. Например, в различных эмпирических исследованиях наблюдались следующие пропорции нулевых значений:

- Число посещений врача: 31% нулевых значений
- Количество арестов: до 89% нулевых значений
- Число патентов: до 63% нулевых значений

Высокая доля нулей может сочетаться с высокими ненулевыми значениями, что создает дополнительные сложности при моделировании.

## Пуассоновская регрессионная модель

### Основная спецификация

Пуассоновская регрессионная модель предполагает, что каждое наблюдение $y_i$ получается из генеральной совокупности, имеющей распределение Пуассона с параметром $\lambda_i$, который связан с регрессорами $x_i$.

**Вероятностная функция**:
$$\text{Prob}(Y_i = y_i | \mathbf{x}_i) = \frac{e^{-\lambda_i}\lambda_i^{y_i}}{y_i!}, \quad y_i = 0, 1, 2, \ldots$$

**Логлинейная модель** (наиболее распространенная спецификация):
$$\ln \lambda_i = \mathbf{x}_i'\boldsymbol{\beta}$$

откуда следует:
$$\lambda_i = e^{\mathbf{x}_i'\boldsymbol{\beta}}$$

### Свойства распределения Пуассона

Ключевой особенностью распределения Пуассона является равенство математического ожидания и дисперсии:

$E(y_i|\mathbf{x}_i) = \text{Var}(y_i|\mathbf{x}_i) = \lambda_i = e^{\mathbf{x}_i'\boldsymbol{\beta}}$

Это означает, что модель Пуассона гетероскедастична по определению.

### Предельные эффекты

**Предельный эффект** для непрерывной переменной $x_j$:
$$\frac{\partial E(y_i|\mathbf{x}_i)}{\partial x_{ij}} = \lambda_i \beta_j$$

**Средний предельный эффект**:
$$N^{-1} \sum_{i=1}^N \frac{\partial E(y_i|\mathbf{x}_i)}{\partial x_{ij}} = \hat{\beta}_j \times N^{-1} \sum_{i=1}^N \exp(\mathbf{x}_i'\hat{\boldsymbol{\beta}}) = \hat{\beta}_j \bar{y}$$

Если переменная $x_j$ выражена в логарифмах, то $\beta_j$ представляет собой эластичность.

## Оценивание модели Пуассона

### Метод максимального правдоподобия

Для оценивания параметров модели Пуассона используется метод максимального правдоподобия.

**Логарифмическая функция правдоподобия**:
$$\ln L = \sum_{i=1}^n \left(-\lambda_i + y_i \mathbf{x}_i'\boldsymbol{\beta} - \ln(y_i!)\right)$$

**Условия первого порядка**:
$$\frac{\partial \ln L}{\partial \boldsymbol{\beta}} = \sum_{i=1}^n (y_i - \lambda_i)\mathbf{x}_i = 0$$

**Гессиан**:
$$\frac{\partial^2 \ln L}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}'} = -\sum_{i=1}^n \lambda_i \mathbf{x}_i \mathbf{x}_i'$$

Гессиан отрицательно определен для всех $\mathbf{x}$ и $\boldsymbol{\beta}$, что гарантирует единственность максимума.

### Оценки и стандартные ошибки

**Оценка ожидаемого числа событий**:
$$\hat{\lambda}_i = \exp(\mathbf{x}_i'\hat{\boldsymbol{\beta}})$$

**Стандартные ошибки** для $\hat{\lambda}_i$ получаются с помощью дельта-метода:
$$\text{Var}(\hat{\lambda}_i) = \hat{\lambda}_i^2 \mathbf{x}_i' \mathbf{V} \mathbf{x}_i$$

где $\mathbf{V}$ — оценка асимптотической матрицы ковариаций для $\boldsymbol{\beta}$.

### Дельта-метод

Дельта-метод позволяет найти асимптотическое распределение функций от оценок параметров. Если $\hat{\boldsymbol{\beta}}$ имеет асимптотически нормальное распределение с математическим ожиданием $\boldsymbol{\beta}$ и дисперсией $\text{Var}(\boldsymbol{\xi})/n$, то для функции $g(\hat{\boldsymbol{\beta}})$:

$$\sqrt{n}(g(\hat{\boldsymbol{\beta}}) - g(\boldsymbol{\beta})) \sim N(0, [g'(\boldsymbol{\beta})]^2 \cdot \text{Var}(\boldsymbol{\xi}))$$

Следовательно, $g(\hat{\boldsymbol{\beta}})$ имеет асимптотически нормальное распределение с математическим ожиданием $g(\boldsymbol{\beta})$ и дисперсией $[g'(\boldsymbol{\beta})]^2 \cdot \text{Var}(\boldsymbol{\xi})/n$.

## Тестирование гипотез

### Основные тесты

Для проверки гипотез в модели Пуассона используются три стандартных теста: Вальда (W), отношения правдоподобия (LR) и множителей Лагранжа (LM).

**Тест Вальда (W)** вычисляется стандартным образом.

**Тест отношения правдоподобия (LR)**:
$$LR = 2\sum_{i=1}^n \ln\left(\frac{\hat{P}_i}{\hat{P}_{\text{restricted},i}}\right)$$

где в знаменателе — вероятность модели с ограничениями.

**Тест множителей Лагранжа (LM)**:
$$LM = \sum_{i=1}^n \mathbf{x}_i'(y_i - \hat{\lambda}_i) \left[\sum_{i=1}^n \mathbf{x}_i \mathbf{x}_i'(y_i - \hat{\lambda}_i)^2\right]^{-1} \sum_{i=1}^n \mathbf{x}_i(y_i - \hat{\lambda}_i)$$

где $\hat{\lambda}_i$ вычисляется с помощью вектора коэффициентов ограниченной модели.

## Измерение качества подгонки модели

### Псевдо-R²

Для модели Пуассона не существует естественного аналога $R^2$, поскольку функция условного среднего не является линейной. Предложены альтернативные меры качества подгонки.

**Стандартизированная мера**:
$$R_p^2 = 1 - \frac{\sum_{i=1}^n \left(\frac{y_i - \hat{\lambda}_i}{\sqrt{\hat{\lambda}_i}}\right)^2}{\sum_{i=1}^n \left(\frac{y_i - \bar{y}}{\sqrt{\bar{y}}}\right)^2}$$

Эта мера может принимать отрицательные значения при отбрасывании переменных.

**Мера Камерона-Виндмейера (1993)**:
$$R_d^2 = 1 - \frac{\sum_{i=1}^n \left[y_i \log\left(\frac{y_i}{\hat{\lambda}_i}\right) - y_i + \hat{\lambda}_i\right]}{\sum_{i=1}^n \left[y_i \log\left(\frac{y_i}{\bar{y}}\right) - y_i + \bar{y}\right]}$$

Альтернативная запись:
$$R_d^2 = \frac{\ell(\hat{\lambda}_i, y_i) - \ell(\bar{y}, y_i)}{\ell(y_i, y_i) - \ell(\bar{y}, y_i)}$$

Эта мера изменяется от 0 до 1 и возрастает при добавлении новых регрессоров.

## Недостатки модели Пуассона

### Основные ограничения

Модель Пуассона имеет несколько существенных недостатков:

1. **Однопараметрическое распределение**: Ограничивает гибкость модели в описании данных
2. **Проблема избыточных нулевых значений**: Предсказанная вероятность нулевых значений часто значительно ниже их фактической доли в выборке
3. **Равенство дисперсии и математического ожидания**: В реальных данных дисперсия обычно превышает среднее (избыточная дисперсия)

### Последствия избыточной дисперсии

Избыточная дисперсия в модели Пуассона приводит к серьезным проблемам:

- Аналогичные последствия с гетероскедастичностью в линейной регрессии
- При наличии усеченных и цензурированных наблюдений оценки коэффициентов становятся несостоятельными
- Существенно завышенные стандартные ошибки по сравнению с обычным ММП
- Необходимость использования робастных оценок дисперсии
- Для оценивания вероятностей требуется больше параметров, чем для условного математического ожидания


## Тестирование избыточной дисперсии

### Тест Камерона-Триведи (1990)

Ограничение модели Пуассона состоит в равенстве дисперсии и математического ожидания $y_i$. Тест Камерона-Триведи проверяет это ограничение:

**Нулевая гипотеза**: $H_0: \text{Var}(y_i) = E(y_i)$

**Альтернативная гипотеза**: $H_1: \text{Var}(y_i) = E(y_i) + \alpha g(E(y_i))$

### Процедура тестирования

Модель избыточной дисперсии:
$$\text{Var}(y_i|\mathbf{x}_i) = \mu_i + \alpha g(\mu_i)$$

где $\alpha$ — неизвестный параметр, а $g(\cdot)$ — известная функция, обычно $g(\mu) = \mu^2$ или $g(\mu) = \mu$.

**Вспомогательная регрессия** (МНК без свободного члена):
$$\frac{(y_i - \hat{\mu}_i)^2 - y_i}{\hat{\mu}_i} = \alpha \frac{g(\hat{\mu}_i)}{\hat{\mu}_i} + u_i$$

где $\hat{\mu}_i = \exp(\mathbf{x}_i'\hat{\boldsymbol{\beta}})$ — предсказанные значения по модели Пуассона. $t$-статистика для коэффициента $\alpha$ асимптотически нормальна при нулевой гипотезе.

## Отрицательная биномиальная модель

### Концепция модели смеси

Отрицательная биномиальная модель является моделью непрерывной смеси, которая позволяет учесть ненаблюдаемую гетерогенность.

**Основные предположения**:

- Случайная величина $y$ следует распределению Пуассона при условии, что параметр $\lambda$ известен: $f(y|\lambda) = \exp(-\lambda)\lambda^y/y!$
- Параметр $\lambda$ содержит случайную компоненту: $\lambda = \mu\nu$
- $\mu$ — детерминированная функция регрессоров: $\mu = \exp(\mathbf{x}_i'\boldsymbol{\beta})$
- $\nu > 0$ — независимо и одинаково распределенная случайная величина с плотностью $g(\nu|\alpha)$


### Функция плотности смеси

Общая формула для плотности смеси распределений:
$$h(y|\mu, \alpha) = \int f(y|\mu, \nu)g(\nu|\alpha)d\nu$$

где $g(\nu|\alpha)$ называется смешиваемым распределением с неизвестным параметром $\alpha$.

### Гамма-распределение как смешиваемое

Пусть $g(\nu) = \frac{\nu^{\delta-1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)}$ — плотность гамма-распределения с $E[\nu] = 1$ и $\text{Var}[\nu] = 1/\delta$.

**Отрицательная биномиальная плотность**:
$$h[y|\mu, \delta] = \int_0^{\infty} \frac{e^{-\mu\nu}(\mu\nu)^y}{y!} \frac{\nu^{\delta-1}e^{-\nu\delta}\delta^\delta}{\Gamma(\delta)} d\nu$$

После интегрирования получаем:
$$h[y|\mu, \alpha] = \frac{\Gamma(\alpha^{-1} + y)}{\Gamma(\alpha^{-1})\Gamma(y+1)} \left(\frac{\alpha^{-1}}{\alpha^{-1} + \mu}\right)^{\alpha^{-1}} \left(\frac{\mu}{\mu + \alpha^{-1}}\right)^y$$

где $\alpha = 1/\delta$.

### Моменты отрицательного биномиального распределения

**Математическое ожидание**:
$$E[y|\mu, \alpha] = \mu$$

**Дисперсия**:
$$\text{Var}[y|\mu, \alpha] = \mu(1 + \alpha\mu)$$

Дисперсия превышает математическое ожидание, поскольку $\alpha > 0$ и $\mu > 0$. Интерпретация угловых коэффициентов такая же, как и для модели Пуассона.

### Типы отрицательных биномиальных моделей

**NB2 модель**: $\text{Var}(y|\alpha, \mu) = \mu + \alpha\mu^2$

**NB1 модель**: $\text{Var}(y|\alpha, \mu) = (1 + \gamma)\mu$, где $\alpha = \gamma/\mu$

В прикладных исследованиях широко применяется модель NB2.

## Выбор между моделями

### Важные принципы

Неверно автоматически переходить к отрицательной биномиальной модели при плохой подгонке Пуассоновской регрессии. Этот механистический подход ошибочен, поскольку проблемы могут заключаться в неправильной спецификации функции условного математического ожидания, которая одинакова для обеих моделей.

**Ключевые соображения**:

- Отрицательная биномиальная модель менее робастна к неправильной спецификации распределения по сравнению с моделью Пуассона
- Выбор смешиваемого распределения остается за исследователем
- Необходимо тщательно анализировать природу данных и теоретические соображения


## Практический пример: эксперимент RAND

### Описание исследования

Анализ данных эксперимента по страхованию здоровья RAND, проводившегося с 1974 по 1982 год. Цель эксперимента — оценить влияние различных методов страхования здоровья (случайно распределенных) на количество визитов к врачу.

**Характеристики выборки**:

- 20,186 наблюдений
- 31% содержат нулевые значения
- Тяжелый правый хвост распределения
- Существенное превышение дисперсии над средним


### Экспериментальный дизайн

- Каждая семья участвовала в одной из 14 различных программ страхования в течение 3-5 лет
- Программы варьировались от бесплатных до 95% соплатежей
- Программы распределялись случайно, что исключает эндогенный эффект воздействия
- Данные включают информацию о визитах к врачу, затратах, демографических характеристиках, состоянии здоровья и типе страховки


### Основные переменные

- **OFFTEC**: Число визитов к врачу офисного типа
- **LC**: Логарифм ставки сострахования (переменная цены)
- **IDP**: Индивидуальный план страхования
- **LPI**: Логарифм годового дохода участника
- **FMDE**: Размер семьи
- **AGE**: Возраст
- **FEMALE**: Пол (женский)
- **CHILD**: Ребенок
- **PHYSLM**: Физические ограничения


### Результаты оценивания

| Модель | Log-likelihood | AIC | BIC |
| :-- | :-- | :-- | :-- |
| Пуассон | -64,426 | 128,870 | 128,944 |
| Отрицательная биномиальная | -27,717 | 55,453 | 55,537 |

Все коэффициенты во всех моделях статистически значимы с робастными стандартными ошибками.

### Интерпретация коэффициента LC

Коэффициент при LC (логарифм ставки сострахования) отражает эффект цены. Чем выше ставка сострахования, тем выше сумма соплатежа пациентом за посещение, следовательно, тем ниже среднее число визитов к врачу. Оценка коэффициента при LC во всех регрессиях значимо отрицательная, что соответствует предсказаниям классической экономической теории.

### Сравнение качества предсказаний

| Модель | Среднее фактическое | Среднее предсказанное | Доля нулей (факт.) | Доля нулей (предск.) |
| :-- | :-- | :-- | :-- | :-- |
| МНК | 3.16 | 3.16 | 0.31 | - |
| Пуассон | 3.16 | 3.16 | 0.31 | 0.18 |
| Отрицательная биномиальная | 3.16 | 3.16 | 0.31 | 0.29 |

Отрицательная биномиальная модель лучше других соответствует данным, особенно в предсказании доли нулевых значений.

## Заключение

Модели счетных данных представляют собой мощный инструментарий для анализа переменных, принимающих неотрицательные целочисленные значения. Пуассоновская регрессия служит базовой моделью, но часто сталкивается с проблемами избыточной дисперсии и избыточных нулевых значений. Отрицательная биномиальная модель предоставляет более гибкую альтернативу, позволяя учесть ненаблюдаемую гетерогенность.

Выбор между моделями должен основываться на тщательном анализе данных, тестировании предположений и теоретических соображениях, а не на механистическом подходе. Правильная спецификация функции условного математического ожидания остается критически важной для обеих моделей. Методы тестирования избыточной дисперсии и оценки качества подгонки помогают исследователям принимать обоснованные решения о выборе наиболее подходящей модели для конкретного набора данных.

# Лекция 10. Временные ряды

## Часть 1. Автокорреляция

### Введение во временные ряды

Временной ряд представляет собой последовательность значений, описывающих протекающий во времени процесс, измеренных в последовательные моменты времени, обычно через равные промежутки. Данные типа временных рядов широко распространены в различных областях экономической деятельности, включая ежедневные цены на акции, курсы валют, еженедельные и месячные объемы продаж, годовые объемы производства.

### Понятие автокорреляции

В контексте анализа временных рядов рассматривается модель:
$$y_t = \alpha + \beta x_t + \varepsilon_t$$

Автокорреляция возникает при нарушении предпосылки теоремы Гаусса-Маркова: $\text{Cov}(\varepsilon_i, \varepsilon_j) \neq 0$ для всех $i \neq j$. Если $\text{Cov}(\varepsilon_t, \varepsilon_{t-1}) \neq 0$, то это автокорреляция первого порядка.

Автокорреляция характерна в основном для временных рядов, поскольку они обладают естественной упорядоченностью, и сегодняшние значения переменных во многом обусловлены их прошлыми значениями. При анализе перекрестных выборок автокорреляция также встречается, но обычно вызвана ошибками спецификации.

### Внешние признаки и последствия автокорреляции

**Внешние признаки автокорреляции:**

- Очень высокие значения $R^2$
- Высокие значения t-статистик
- Несущественные переменные кажутся значимыми

**Последствия автокорреляции:**

- Оценки МНК самих коэффициентов остаются несмещенными
- Оценки стандартных ошибок коэффициентов становятся смещенными
- t-статистики становятся неадекватными
- Оценки МНК становятся неэффективными


### Методы диагностики автокорреляции

Для диагностики автокорреляции проверяется гипотеза $H_0: \rho = 0$, где $\rho = \text{corr}(\varepsilon_t, \varepsilon_{t-1})$.

#### Графический анализ

Графический метод включает построение графика остатков от номера наблюдения или от предыдущего значения остатков:

- При отсутствии автокорреляции точки распределены случайно
- При положительной автокорреляции ($\rho > 0$) наблюдается групповая структура остатков
- При отрицательной автокорреляции ($\rho < 0$) остатки быстро меняют знак


#### Статистика Дарбина-Уотсона

Статистика Дарбина-Уотсона определяется как:
$$d = \frac{\sum_{t=2}^T (e_t - e_{t-1})^2}{\sum_{t=1}^T e_t^2}$$

где $T$ — число наблюдений, $e_t$ — остатки уравнения регрессии.

**Для тестирования положительной автокорреляции:**

- $H_0: \rho = 0$ (отсутствие положительной автокорреляции)
- $H_A: \rho > 0$ (наличие положительной автокорреляции)

**Решающее правило:**

- Если $d < d_L$, то гипотеза $H_0$ отвергается
- Если $d > d_U$, то гипотеза $H_0$ не отвергается
- Если $d_L \leq d \leq d_U$, то ситуация остается неопределенной

**Для тестирования отрицательной автокорреляции:**

- Если $d > 4 - d_L$, то гипотеза $H_0$ отвергается
- Если $d < 4 - d_U$, то гипотеза $H_0$ не отвергается
- Если $4 - d_U \leq d \leq 4 - d_L$, то ситуация неопределенная


#### Ограничения теста Дарбина-Уотсона

Тест DW нельзя применять в следующих случаях:

- В уравнении регрессии отсутствует свободный член
- Имеется стохастический регрессор (например, $Y_{t-1}$)
- Случайные ошибки удовлетворяют авторегрессионной схеме не первого, а более высокого порядка


#### Тест Бройша-Годфри

Тест Бройша-Годфри проверяет гипотезу об отсутствии автокорреляции порядка $p$ и выявляет значимость зависимости остатков от их предшествующих значений во вспомогательной регрессии:

$$e_t = \alpha_0 + \sum_{j=1}^k \alpha_j X_{jt} + \sum_{i=1}^p \beta_i e_{t-i} + u_t$$

При нулевой гипотезе $nR^2 \sim \chi^2(p)$, где $p$ — число лагов остатков во вспомогательной регрессии.

### Способы устранения автокорреляции

Основные методы устранения автокорреляции включают:

- Использование стандартных ошибок в форме Ньюи-Веста (HAC-ошибки)
- Оценивание обобщенным МНК или методом максимального правдоподобия с ошибками, подчиняющимися случайному процессу ARMA(p,q)


## Часть 2. Анализ одномерных временных рядов

### Модели временных рядов

#### AR модели (авторегрессии)

**AR(1) модель:**
$$y_t = \alpha_0 + \alpha_1 y_{t-1} + \varepsilon_t$$

**AR(p) модель:**
$$y_t = \alpha_0 + \alpha_1 y_{t-1} + \cdots + \alpha_p y_{t-p} + \varepsilon_t$$

где $t = 1, \ldots, T$, $\varepsilon_t$ — процесс белого шума с $E(\varepsilon_t) = 0$, $\text{Var}(\varepsilon_t) = \sigma_\varepsilon^2$, $\text{Cov}(\varepsilon_t, \varepsilon_{t-s}) = 0$ для любых целых $s > 0$.

#### Лаговые операторы

Лаговый оператор определяется как:
$$L Y_t = Y_{t-1}$$
$$L^s Y_t = Y_{t-s}$$

AR(1) модель можно записать как:
$$y_t = \theta y_{t-1} + \varepsilon_t \Leftrightarrow (1 - \theta L)y_t = \varepsilon_t$$

AR(p) модель:
$$\theta(L)y_t = \varepsilon_t$$
где $\theta(L) = 1 - \theta_1 L - \theta_2 L^2 - \cdots - \theta_p L^p$

#### MA модели (скользящее среднее)

**MA(1) модель:**
$$y_t = \alpha_0 + \varepsilon_t + \beta_1 \varepsilon_{t-1}$$

**MA(q) модель:**
$$y_t = \alpha_0 + \varepsilon_t + \beta_1 \varepsilon_{t-1} + \cdots + \beta_q \varepsilon_{t-q}$$

где $\varepsilon_t$ — процесс белого шума.

### Понятие стационарности

#### Строгая стационарность

Случайный процесс называется строго стационарным, если сдвиг во времени не меняет ни одну из функций плотности распределения: $f_n(x_{t_1}, \ldots, x_{t_n}) = f_n(x_{t_1+\Delta}, \ldots, x_{t_n+\Delta})$ для всех $n$, моментов времени $t_1, \ldots, t_n$ и целочисленных $\Delta$.

#### Слабая стационарность

Случайный процесс называется слабо стационарным (стационарным в широком смысле), если:

- Математическое ожидание и дисперсия существуют и не зависят от времени
- Автокорреляционная функция зависит только от разности значений $(t_1 - t_2)$


#### Стационарность MA процессов

Любой MA(q) процесс является стационарным. Это можно показать на примере:
$$E(5 + u_t + 0.6u_{t-1} + 0.2u_{t-2}) = 5$$

Ковариация определяется совпадающими индексами, и при изменении $t$ совпадающие по индексам пары остаются теми же.

### Автокорреляционная функция (ACF)

Автокорреляционная функция между $y_t$ и $y_{t-s}$ определяется как:
$$\rho_s = \frac{\gamma_s}{\gamma_0}$$

где $\gamma_s = \text{cov}(y_t, y_{t-s})$.

#### ACF для AR(1) процесса

Для процесса $y_t = \phi_0 + \phi_1 y_{t-1} + \nu_t$:
$$\gamma_0 = \text{var}(y_t) = \frac{\sigma_\nu^2}{1-\phi_1^2}$$
$$\gamma_s = \text{cov}(y_t, y_{t-s}) = \frac{\sigma_\nu^2 \phi_1^s}{1-\phi_1^2}$$

Следовательно:
$$\rho_0 = 1, \quad \rho_1 = \phi_1, \quad \rho_2 = \phi_1^2, \quad \ldots, \quad \rho_s = \phi_1^s$$

Автокорреляционная функция AR(1) процесса экспоненциально убывает.

#### ACF для MA(1) процесса

Для процесса $y_t = \nu_t + \psi_1 \nu_{t-1}$:
$$\rho_0 = 1, \quad \rho_1 = \frac{\psi_1}{1+\psi_1^2}, \quad \rho_s = 0 \text{ для всех } s \geq 2$$

Для MA(1) процесса только ACF первого лага ненулевая.

### Частная автокорреляционная функция (PACF)

$s$-ым значением частной автокорреляционной функции $\varphi_{ss}$ слабостационарного случайного процесса $y_t$ называется $s$-ый коэффициент следующей регрессии:
$$y_t^* = \varphi_{s1}y_{t-1}^* + \cdots + \varphi_{ss}y_{t-s}^* + v_t$$

где $y_t^* = y_t - E(y_t)$.

### Доверительные интервалы для ACF и PACF

Если процесс полностью случаен, то все нормализованные оцененные автокорреляции асимптотически нормально распределены:
$$\sqrt{T}\hat{\rho}_j \xrightarrow{d} N(0,1) \Rightarrow \left[-\frac{1.96}{\sqrt{T}}, \frac{1.96}{\sqrt{T}}\right]$$

является 95%-ным доверительным интервалом.

### Белый шум

Процесс $\varepsilon_t$, удовлетворяющий условиям $E(\varepsilon_t) = 0$, $\text{Var}(\varepsilon_t) = \sigma^2$, $\text{Cov}(\varepsilon_i, \varepsilon_j) = 0$ при $i \neq j$, называется белым шумом. Если добавить требование нормального распределения, то процесс называется гауссовым белым шумом: $\varepsilon_t \sim WN(0, \sigma^2)$.

#### Тест Льюнг-Бокса

Для тестирования гипотезы о белом шуме используется тест Льюнг-Бокса:

- $H_0: \rho_1 = \rho_2 = \cdots = \rho_k = 0$
- $H_1:$ в обратном случае

**Статистика Бокса-Пирса:**
$$Q = T\sum_{j=1}^k \hat{\rho}_j^2 \sim \chi^2(k)$$

**Статистика Льюнг-Бокса:**
$$Q^*_{LB} = T(T+2)\sum_{j=1}^k \frac{1}{T-j}\hat{\rho}_j^2 \sim \chi^2(k)$$

### Теорема Вольда

Если $y_t$ — стационарный процесс, то он представим в виде:
$$y_t = \sum_{i=0}^{\infty} \alpha_i u_{t-i} + r_t$$

где:

- $u_t$ — белый шум
- $\sum \alpha_i^2 < \infty$
- $r_t$ — линейно предсказуемый случайный процесс
- $\text{Cov}(u_t, r_t) = 0$


### Обратимость MA процессов

#### Лаговый многочлен MA(q) части

Для процесса $y_t = 5 + u_t + 0.6u_{t-1} + 0.2u_{t-2}$ лаговый многочлен:
$P(L) = 1 + 0.6L + 0.2L^2$

#### Характеристический многочлен

Характеристический многочлен для того же процесса:
$$\phi(\lambda) = \lambda^2 + 0.6\lambda + 0.2$$

**Связь многочленов:**
$$P(x) = x^q \cdot \phi(1/x)$$

#### Условие обратимости

**Характеристический вариант:** Уравнение MA(q) процесса удовлетворяет условию обратимости, если у характеристического многочлена $\phi(\lambda)$ все корни $\lambda_i < 1$.

**Лаговый вариант:** Уравнение MA(q) процесса удовлетворяет условию обратимости, если у лагового многочлена $P(L)$ все корни $\ell_i > 1$.

### ARMA процессы

#### Определение ARMA процесса

ARMA(p,q) процессом с уравнением:
$y_t = c + \beta_1 y_{t-1} + \cdots + \beta_p y_{t-p} + u_t + \alpha_1 u_{t-1} + \cdots + \alpha_q u_{t-q}$

где $u_t$ — белый шум, $\beta_p \neq 0$ и $\alpha_q \neq 0$, называется решение этого уравнения вида MA($\infty$) относительно $u_t$.

#### Теорема о стационарности ARMA

Если ARMA уравнение $P(L)y_t = c + Q(L)u_t$ несократимо, то оно имеет ровно одно стационарное решение вида MA($\infty$) относительно $u_t$ тогда и только тогда, когда $\ell_i > 1$ для всех корней лагового полинома $P(L)$.

### Нестационарные временные ряды

#### Типы нестационарных процессов

**TS процессы (Trend Stationary):** Процесс вида $X_t = \alpha + \beta t + \varepsilon_t$, приводимый к стационарному путем выделения линейного тренда.

**DS процессы (Difference Stationary):** Процесс вида $X_t = X_{t-1} + \varepsilon_t$, приводимый к стационарному путем взятия первой разности.

#### Характеристики процессов

| Процесс | TSP | DSP |
| :-- | :-- | :-- |
| Причина нестационарности | Непостоянный тренд | Непостоянная дисперсия |
| Память о шоках | Конечная | Бесконечная |
| Влияние шоков | Ослабевает со временем | Постоянное |

### Модели ARIMA(p,d,q)

ARIMA(p,d,q) модель записывается как:
$$(1-L)^d (1-\alpha_1 L - \cdots - \alpha_p L^p)y_t = \alpha_0 + (1 + \beta_1 L + \cdots + \beta_q L^q)\varepsilon_t$$

Лаговый оператор разности: $\Delta x_t = (1-L)x_t$.

### Подход Бокса-Дженкинсона

Методология включает четыре этапа:

1. **Идентификация параметров** ARIMA(p,d,q) с использованием коррелограмм
2. **Оценивание** методом максимального правдоподобия или МНК
3. **Тестирование и выбор спецификации** с помощью информационных критериев
4. **Прогнозирование**

#### Алгоритм выбора числа лагов

1. Выбрать максимальное число лагов $p_{\max}$ и $q_{\max}$ на основе ACF и PACF
2. Оценить модели со всеми возможными комбинациями $p = 0, \ldots, p_{\max}$ и $q = 0, \ldots, q_{\max}$
3. Выбрать модель с наименьшим значением информационного критерия

### Тесты на единичные корни

#### Тест Дики-Фуллера (ADF)

Для модели $X_t = \alpha + \rho X_{t-1} + \varepsilon_t$ тестируются гипотезы:

- $H_0: \rho = 1$ (DS процесс)
- $H_1: \rho < 1$ (TS процесс)

После преобразования $\Delta X_t = \alpha + (\rho-1)X_{t-1} + \varepsilon_t$ с $\gamma = \rho-1$:

- $H_0: \gamma = 0$
- $H_1: \gamma < 0$


#### Процедура Доладо-Дженкинсона-Сосвилла-Ривьеро

Многоступенчатая процедура тестирования единичных корней:

$$\Delta y_t = c + \beta t + \gamma y_{t-1} + \sum_{i=1}^{p-1} c_i \Delta y_{t-i} + \varepsilon_t$$

1. Оценить модель с константой и трендом, провести тест Дики-Фуллера
2. При нестационарности проверить значимость тренда
3. При незначимости тренда — тест с константой без тренда
4. При нестационарности проверить значимость константы
5. При незначимости константы — тест без константы и тренда

### Прогнозирование

Основное значение ARMA и ARIMA моделей заключается в прогнозировании. Используется динамический прогноз, где для построения прогноза на несколько шагов вперед применяются ранее полученные прогнозные значения.

#### Критерии качества прогноза

**Root Mean Squared Error:**
$$RMSE = \sqrt{\frac{\sum_{t=T+1}^{T+h} (\hat{y}_t - y_t)^2}{h+1}}$$

**Mean Absolute Error:**
$$MAE = \frac{\sum_{t=T+1}^{T+h} |\hat{y}_t - y_t|}{h+1}$$

**Mean Absolute Percent Error:**
$$MAPE = \frac{\sum_{t=T+1}^{T+h} \frac{|\hat{y}_t - y_t|}{y_t}}{h+1}$$

где $h$ — длина интервала прогнозирования, $\hat{y}_t$ — прогнозное значение, $y_t$ — истинное значение.


